import os
import torch
import difflib
from os.path import basename, isfile
from os import makedirs
from glob import glob
import networkx as nx
import pickle
import json
from texttable import Texttable
from functools import lru_cache
import re
from collections import deque
from SPARQLWrapper import SPARQLWrapper, JSON, XML
from openai import OpenAI
import time
import sqlite3
from urllib.error import HTTPError
from threading import Lock
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig


SPARQLPATH = "http://localhost:9890/sparql"  # depend on your own internal address and port, shown in Freebase folder's readme.md


sparql_head_relations = """\nPREFIX ns: <http://rdf.freebase.com/ns/>\nSELECT ?relation\nWHERE {\n  ns:%s ?relation ?x .\n}"""
sparql_tail_relations = """\nPREFIX ns: <http://rdf.freebase.com/ns/>\nSELECT ?relation\nWHERE {\n  ?x ?relation ns:%s .\n}"""
sparql_tail_entities_extract = """PREFIX ns: <http://rdf.freebase.com/ns/>\nSELECT ?tailEntity\nWHERE {\nns:%s ns:%s ?tailEntity .\n}""" 
sparql_head_entities_extract = """PREFIX ns: <http://rdf.freebase.com/ns/>\nSELECT ?tailEntity\nWHERE {\n?tailEntity ns:%s ns:%s  .\n}"""
sparql_id = """PREFIX ns: <http://rdf.freebase.com/ns/>\nSELECT DISTINCT ?tailEntity\nWHERE {\n  {\n    ?entity ns:type.object.name ?tailEntity .\n    FILTER(?entity = ns:%s)\n  }\n  UNION\n  {\n    ?entity <http://www.w3.org/2002/07/owl#sameAs> ?tailEntity .\n    FILTER(?entity = ns:%s)\n  }\n}"""


sparql_tail_entities_and_relations = """
PREFIX ns: <http://rdf.freebase.com/ns/>
SELECT ?relation ?tailEntity
WHERE {
    ns:%s ?relation ?tailEntity .
}
"""

sparql_head_entities_and_relations = """
PREFIX ns: <http://rdf.freebase.com/ns/>
SELECT ?relation ?headEntity
WHERE {
    ?headEntity ?relation ns:%s .
}
"""



def tab_printer(args):
    """
    Function to print the logs in a nice tabular format.
    :param args: Parameters used for the model.
    """
    args = vars(args)
    keys = sorted(args.keys())
    t = Texttable()
    rows = [["Parameter", "Value"]] + [[k.replace("_", " ").capitalize(), args[k]] for k in keys]
    t.add_rows(rows)
    print(t.draw())



def initialize_large_database(db_path):
    # connect to the SQLite database file specified by db_path
    with sqlite3.connect(db_path) as conn:
        cursor = conn.cursor()  # create cursor, which allows SQL commands to be withini the DB

        # create a table named subgraphs
        #   question_id: stores the ID of the question being processed
        #   chunk_index: chunk number for large subgraph data is split into multiple pieces
        #   data: stores the actual subgraph information in binary format
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS subgraphs (
            question_id TEXT,
            chunk_index INTEGER,
            data BLOB,
            PRIMARY KEY (question_id, chunk_index)
        )
        ''')
        conn.commit()   # save the changes to the SQL DB



def retry_operation(func):
    """Decorator to retry database operations."""
    def wrapper(*args, **kwargs):
        attempts = 0
        max_retries = kwargs.pop('max_retries', 300)
        wait_time = kwargs.pop('wait_time', 6)
        while attempts < max_retries:
            try:
                return func(*args, **kwargs)
            except sqlite3.OperationalError as e:
                if 'database is locked' in str(e):
                    print(f"Database is locked, retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                    attempts += 1
                else:
                    print("An error occurred:", e)
                    break
        print("Failed after several attempts.")
        return None
    return wrapper

@retry_operation
def delete_data_by_question_id(db_path, question_id):
    with sqlite3.connect(db_path) as conn:
        start = time.time()
        cursor = conn.cursor()
        cursor.execute('DELETE FROM subgraphs WHERE question_id = ?', (question_id,))
        conn.commit()
        # print(f"Data associated with question_id {question_id} has been deleted.")
        # print(f"Time taken to delete data: {time.time() - start} seconds.")

@retry_operation
def save_to_large_db(db_path, question_id, data_dict, chunk_size=256 * 1024 * 1024):
    with sqlite3.connect(db_path) as conn:
        start = time.time()
        cursor = conn.cursor()
        data_blob = pickle.dumps(data_dict)
        for i in range(0, len(data_blob), chunk_size):
            chunk = data_blob[i:i+chunk_size]
            cursor.execute('INSERT INTO subgraphs (question_id, data, chunk_index) VALUES (?, ?, ?)',
                           (question_id, chunk, i // chunk_size))
            conn.commit()
        # print("Data saved to database.")
        # print(f"Time taken to save data: {time.time() - start} seconds.")

# Retrieves and reconstructs stored data from an SQLite database based on a question_id
@retry_operation
def load_from_large_db(db_path, question_id):
    with sqlite3.connect(db_path) as conn:  # connect with DB
        start = time.time()
        cursor = conn.cursor()

        # Retrieves the "data" column of records where "question_id" matches
        cursor.execute('SELECT data FROM subgraphs WHERE question_id = ? ORDER BY chunk_index', (question_id,))
        
        # Concatenates all retrieved data values into a single binary blob
        data_blob = b''.join(row[0] for row in cursor if row[0] is not None)
        
        if not data_blob:
            # print("No data found or data is empty.")
            return None
        
        # Deserializes (unpickles) the binary blob back into its original Python object.
        try:
            result = pickle.loads(data_blob)
            return result
        except EOFError as e:
            print(f"EOFError when unpickling data: {e}")
            return None


def query_label_from_kg(mid, sparql_endpoint="http://localhost:9890/sparql"):
    sparql = SPARQLWrapper(sparql_endpoint)
    query = f"""
    PREFIX ns: <http://rdf.freebase.com/ns/>
    SELECT ?name
    FROM <http://freebase.com>
    WHERE {{
        ns:{mid} ns:type.object.name ?name .
        FILTER (lang(?name) = "en")
    }}
    """
    sparql.setQuery(query)
    sparql.setReturnFormat(JSON)
    try:
        result = sparql.query().convert()
        bindings = result.get("results", {}).get("bindings", [])
        return bindings[0]["name"]["value"] if bindings else None
    except:
        return None



def prepare_dataset(dataset_name):
    if dataset_name == 'cwq':
        with open('../data/cwq.json',encoding='utf-8') as f:
            datas = json.load(f)
        question_string = 'question'
        ID = 'ID'

    elif dataset_name == 'cwq_multi':
        with open('../data/cwq_multi.json',encoding='utf-8') as f:
            datas = json.load(f)
        ID = 'ID'
        question_string = 'question'

    elif dataset_name == 'webqsp':
        with open('../data/WebQSP.json',encoding='utf-8') as f:
            datas = json.load(f)
        question_string = 'RawQuestion'
        ID = "QuestionId"
        # answer = ""

    elif dataset_name == 'webqsp_multi':
        with open('../data/webqsp_multi.json',encoding='utf-8') as f:
            datas = json.load(f)
        question_string = 'question'

    elif dataset_name == 'grailqa':
        with open('../data/grailqa.json',encoding='utf-8') as f:
            datas = json.load(f)
        question_string = 'question'
        ID = 'qid'
        
    elif dataset_name == 'simpleqa':
        with open('../data/SimpleQA.json',encoding='utf-8') as f:
            datas = json.load(f)    
        question_string = 'question'
        ID = 'question'
        answer_string = 'answer'

    elif dataset_name == 'webquestions':
        with open('../data/WebQuestions.json',encoding='utf-8') as f:
            datas = json.load(f)
        ID = 'question'
        question_string = 'question'

    else:
        print("dataset not found, you should pick from {cwq, webqsp, grailqa, simpleqa, qald, webquestions, trex, zeroshotre, creak}.")
        exit(-1)
    return datas, question_string, ID



def check_answerlist(dataset_name, origin_data, topic_entity):
    final_answer_list = []
    answer_list = []
    # origin_data = [j for j in ground_truth_datas if j[question_string] == ori_question]
    if dataset_name == 'cwq':
        for k, v in origin_data["answer"].items():
            answer_list.append(
                {"mid": k, "label": v}
            )

    elif dataset_name == 'webqsp':
        answers = origin_data["Parses"]
        for parse in answers:
            for ans in parse['Answers']:
                if ans.get("AnswerType") == "Entity":
                    mid = ans.get("AnswerArgument")
                    label = query_label_from_kg(mid)
                    if label:
                        answer_list.append(
                            {"mid": mid, "label": label}
                        )

    elif dataset_name == 'grailqa':
        answers = origin_data["answer"]
        for answer in answers:
            if answer.get("answer_type") == "Entity":
                mid = answer["answer_argument"]
                label = query_label_from_kg(mid)
                if label:
                    answer_list.append(
                        {"mid": mid, "label": label}
                    )

    elif dataset_name == 'simpleqa':
        for k,v in origin_data["real_answer"].items():
            answer_list.append(
                {"mid": k, "label": v, "relation": origin_data["relation"]}
            )

    elif dataset_name == 'webquestions':
        for k,v in origin_data["real_answers"].items():
            answer_list.append(
                {"mid": k, "label": v["label"], "relation": v["relation"]}
            )

    return [dict(t) for t in {tuple(sorted(d.items())) for d in answer_list}]



def initialize_large_database(db_path):
    # connect to the SQLite database file specified by db_path
    with sqlite3.connect(db_path) as conn:
        cursor = conn.cursor()  # create cursor, which allows SQL commands to be withini the DB

        # create a table named subgraphs
        #   question_id: stores the ID of the question being processed
        #   chunk_index: chunk number for large subgraph data is split into multiple pieces
        #   data: stores the actual subgraph information in binary format
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS subgraphs (
            question_id TEXT,
            chunk_index INTEGER,
            data BLOB,
            PRIMARY KEY (question_id, chunk_index)
        )
        ''')
        conn.commit()   # save the changes to the SQL DB



# model_id: Huggingface æ¨¡å‹çš„å­—ç¬¦ä¸²è·¯å¾„ï¼Œä¾‹å¦‚ "openchat/openchat-3.5-0106"
# cache_dir: æœ¬åœ°æ¨¡å‹ç¼“å­˜ç›®å½•ï¼Œå°†æ¨¡å‹ä¸‹è½½å¹¶ä¿å­˜åœ¨è¿™é‡Œ
def init_LLM(model_id: str, cache_dir: str = "/data1/zhuom/LLM"):
    """
    åŠ è½½æŒ‡å®šæ¨¡å‹ï¼Œè‡ªåŠ¨é€‚é… prompt å¹¶æ‰§è¡Œæµ‹è¯•ã€‚
    æ”¯æŒå¤šä¸ª Huggingface æœ¬åœ°æ¨¡å‹ã€‚
    """
    print(f"ğŸ“¦ å‡†å¤‡åŠ è½½æ¨¡å‹: {model_id}")

    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        cache_dir=cache_dir,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # ========= Prompt æ„é€ å™¨ =========
    def format_prompt(user_input: str) -> str:
        # ChatML æ ·å¼ï¼ˆOpenChat, Mistral, Nous, Mixtral, Yi, Qwen ç­‰ï¼‰
        if any(key in model_id.lower() for key in [
            "openchat", "mistral", "mixtral", "nous", "zephyr", "starling", "yi", "qwen", "gemma"
        ]):
            return (
                "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
                f"<|im_start|>user\n{user_input}<|im_end|>\n"
                "<|im_start|>assistant\n"
            )
        # LLaMA 2/3 ç³»åˆ— INST æ ¼å¼
        elif "llama" in model_id.lower():
            return f"[INST] <<SYS>>\nYou are a helpful assistant.\n<</SYS>>\n\n{user_input} [/INST]"
        else:
            raise NotImplementedError(f"âŒ æœªæ”¯æŒè¯¥æ¨¡å‹çš„ prompt æ ¼å¼: {model_id}")

    # ========= è‡ªåŠ¨é€‰æ‹©æµ‹è¯•é—®é¢˜ =========
    test_question = {
        "openchat/openchat-3.5-0106": "What is the capital of France?",
        "mistralai/Mistral-7B-Instruct-v0.3": "What is the capital of China?",
        "NousResearch/Nous-Hermes-2-Mistral-7B-DPO": "What is the capital of Japan?",
        "mistralai/Mixtral-8x7B-Instruct-v0.1": "What are the main causes of climate change?",
        "meta-llama/Llama-2-13b-chat-hf": "Who wrote the novel '1984'?",
        "meta-llama/Meta-Llama-3-8B-Instruct": "Explain how a neural network learns.",
        "google/gemma-7b-it": "What is the boiling point of water?",
        "HuggingFaceH4/zephyr-7b-beta": "How does photosynthesis work?",
        "berkeley-nest/Starling-LM-7B-alpha": "What is the tallest mountain on Earth?",
        "Qwen/Qwen1.5-7B-Chat": "How many continents are there?",
        "01-ai/Yi-1.5-9B-Chat": "What is the Pythagorean theorem?"
    }.get(model_id, "What is the capital of France?")  # é»˜è®¤ fallback

    prompt = format_prompt(test_question)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # è®¾ç½®ç”Ÿæˆé…ç½®ï¼ˆé¿å… warningï¼‰
    gen_config = GenerationConfig.from_model_config(model.config)
    gen_config.do_sample = True
    gen_config.temperature = 0.4
    gen_config.top_p = 0.9
    gen_config.max_new_tokens = 128
    gen_config.pad_token_id = tokenizer.eos_token_id

    print(f"ğŸ§  æµ‹è¯•é—®é¢˜: {test_question}")
    with torch.no_grad():
        outputs = model.generate(**inputs, generation_config=gen_config)

    # è§£ç å¹¶åªä¿ç•™ç¬¬ä¸€è½® assistant å›ç­”
    response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    if "<|im_end|>" in response:
        response = response.split("<|im_end|>")[0]
    elif "[/INST]" in prompt and "###" in response:
        response = response.split("###")[0]
    print("\nğŸ’¬ æ¨¡å‹å›å¤ï¼š")
    print(response.strip())

    return model, tokenizer



def run_LLM(prompt, model_name, model=None, tokenizer=None, temperature=0.4):
    result = ''

    # ========== OpenAI GPT æ¨¡å‹ ==========
    if "gpt" in model_name.lower():             # openai_api_base = "http://localhost:8000/v1"
        openai_api_key = "your_api_keys"
        if model_name == "gpt4":
            model_engine = "gpt-4-turbo"
        else:
            model_engine = "gpt-3.5-turbo-0125"

        client = OpenAI(api_key=openai_api_key)     # create an OpenAI client for sending request

        # æ„é€  OpenAI çš„ Chat è¯·æ±‚æ ¼å¼ï¼ˆmessages = [system msg + user prompt]ï¼‰
        system_message = "You are an AI assistant that helps people find information."
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": prompt}
        ]

        # ç”¨æ¥å£å‘èµ·å¯¹è¯ï¼Œæœ€å¤šå°è¯• 3 æ¬¡ï¼Œæ¯æ¬¡å¤±è´¥é—´éš” 2 ç§’ã€‚
        try_time = 0
        while try_time < 3:
            try:
                response = client.chat.completions.create(
                    model=model_engine,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=512,
                    frequency_penalty=0,
                    presence_penalty=0
                )
                result = response.choices[0].message.content
                break
            except Exception as e:
                print(f"OpenAI error: {e}")
                print("Retrying in 2 seconds...")
                try_time += 1
                time.sleep(2)
                
    # ========== æœ¬åœ°æ¨¡å‹ ==========
    else:
        if model is None or tokenizer is None:
            raise ValueError("æœ¬åœ°æ¨¡å‹è°ƒç”¨æ—¶å¿…é¡»ä¼ å…¥ model å’Œ tokenizer")

        # === Prompt åŒ…è£…ï¼šæ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ ¼å¼ ===
        model_name_lower = model_name.lower()

        if any(key in model_name_lower for key in [
            "openchat", "mistral", "mixtral", "nous", "zephyr", "starling", "yi", "qwen", "gemma"
        ]):
            prompt = (
                "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
                f"<|im_start|>user\n{prompt}<|im_end|>\n"
                "<|im_start|>assistant\n"
            )
        elif "llama" in model_name_lower:
            # LLaMA 2/3 æ ¼å¼
            prompt = f"[INST] <<SYS>>\nYou are a helpful assistant.\n<</SYS>>\n\n{prompt} [/INST]"
        else:
            raise NotImplementedError(f"âŒ æœªæ”¯æŒçš„æœ¬åœ°æ¨¡å‹ prompt æ ¼å¼: {model_name}")

        # Tokenize
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        # Generate config
        gen_config = GenerationConfig.from_model_config(model.config)
        gen_config.do_sample = True
        gen_config.temperature = temperature
        gen_config.top_p = 0.9
        gen_config.max_new_tokens = 512
        gen_config.pad_token_id = tokenizer.eos_token_id

        # Generate
        with torch.no_grad():
            outputs = model.generate(**inputs, generation_config=gen_config)

        # Decode output
        decoded = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)

        # æˆªæ–­æ¨¡å‹å›å¤ï¼Œåªä¿ç•™ç¬¬ä¸€è½®
        if "<|im_end|>" in decoded:
            result = decoded.split("<|im_end|>")[0].strip()
        elif "[/INST]" in prompt and "###" in decoded:
            result = decoded.split("###")[0].strip()
        else:
            result = decoded.strip()

    print(f"\n{model_name} response:\n{result}\n")
    return result



# ä» LLM è¿”å›çš„æ–‡æœ¬ï¼ˆåŒ…å«æ¨ç†é“¾ CoTï¼‰ä¸­æå–:
# - æ¨ç†è·¯å¾„é•¿åº¦ï¼ˆpath_lengthï¼‰ï¼šç”¨äºé¢„æµ‹å›¾æ¢ç´¢èµ·å§‹æ·±åº¦ D_predict
# - æ¨ç†é“¾æ–‡æœ¬ï¼ˆthinking_cot_lineï¼‰ï¼šç”¨äºæŒ‡å¯¼è·¯å¾„æ’åºã€å®ä½“æ’åºç­‰ã€‚
def extract_path_length_from_text(text):
    back_up = text

    # åŸºäº " - " æ‹†åˆ†å…³ç³»é“¾ï¼ˆå®ä½“-å…³ç³»-å®ä½“...ï¼‰
    tokens = re.split(r'\s*-\s*', text.strip())

    # è®¡ç®—è·¯å¾„é•¿åº¦
    # æ¯æ¡è·¯å¾„ç»“æ„æ˜¯ï¼šEntity - Relation - Entityï¼Œæ‰€ä»¥ æ¯è·³éœ€è¦ä¸¤ä¸ª tokenï¼ˆä¸€ä¸ªå…³ç³»å’Œä¸€ä¸ªç›®æ ‡å®ä½“ï¼‰
    path_length = (len(tokens) - 1) // 2

    # eæŠ½å–æ¨ç†é“¾ CoT è¡Œ
    match = re.search(r'cot\s*:\s*(.*)', back_up, re.IGNORECASE)

    # è¾“å‡ºç»“æœ
    if match:
        thinking_cot_line = match.group(1).strip()
        # print('æå–çš„æ–‡æœ¬æ˜¯ï¼š')
        # print(thinking_cot_line)
    else:
        print('cannot find the cot line')

    return path_length, thinking_cot_line


# ä» LLM çš„è¿”å›æ–‡æœ¬ä¸­æå– æ‹†åˆ†åçš„å­é—®é¢˜ï¼ˆsplit questionsï¼‰
# Return: ["question1", "question2", ...]
def extract_split_questions(text):
    lines = text.strip().split('\n')
    questions = []  # store extracted split questions

    for line in lines:
        line_no_spaces = line.replace(' ', '')

        # å¦‚æœè¯¥è¡ŒåŒ…å« splitï¼Œæå–å†’å·åé¢çš„å­é—®é¢˜
        if re.search(r'split', line_no_spaces, re.IGNORECASE):
            parts = line.split(':', 1)
            if len(parts) > 1:
                question = parts[1].strip()
                questions.append(question)
            else:
                # å¦‚æœæ²¡æœ‰ ':'ï¼Œæ•´ä¸ªè¡Œä½œä¸ºé—®é¢˜
                questions.append(line.strip())

    return questions


# æ ¹æ® LLM ç”Ÿæˆçš„æ¨ç†é“¾ï¼ˆCoT reasoning lineï¼‰ï¼Œå¯¹ topic entities è¿›è¡Œé¡ºåºé‡æ’ï¼Œä»¥åŒ¹é… LLM çš„æ€ç»´é¡ºåºã€‚
def reorder_entities(cot_line, topic_entity_dict):
    entity_positions = []   # ç”¨äºå­˜å‚¨ (position, entity_name) å…ƒç»„ã€‚

    for entity in topic_entity_dict:
        score, position = find_best_matching_substring(entity, cot_line)

        # Assign a high position if no match is found
        if position != -1:
            entity_positions.append((position, entity))
        else:
            entity_positions.append((float('inf'), entity)) # assign (inf, entity) if not found

    # Sort entities based on their positions in cot_line
    entity_positions.sort()

    sorted_entities = [entity for position, entity in entity_positions]
    return sorted_entities


# Finds the best matching substring of an entity within a given CoT-line.
# Returning: the best match (highest similarity score and starting position)
def find_best_matching_substring(entity, cot_line):
    len_entity = len(entity)
    len_cot = len(cot_line)

    # Consider substrings within reasonable lengths
    min_len = max(1, len_entity // 2)
    max_len = min(len_cot, len_entity * 2)

    best_score = 0
    best_start = -1

    for length in range(min_len, max_len + 1):      # iterate over all string length
        for start in range(len_cot - length + 1):   # sliding window over 
            substring = cot_line[start:start + length]
            score = difflib.SequenceMatcher(None, entity, substring).ratio()    # compute similarity ratio
            if score > best_score:
                best_score = score
                best_start = start

    return best_score, best_start



# ç»™å®šå®ä½“ IDï¼ˆå¦‚ "m.02jx3"ï¼‰ï¼Œé€šè¿‡ SPARQL æŸ¥è¯¢çŸ¥è¯†å›¾è°±ï¼Œè¿”å›è¿™ä¸ªå®ä½“çš„åç§°ï¼ˆä¾‹å¦‚ "Barack Obama"ï¼‰
# ä½¿ç”¨ç¼“å­˜åŠ é€ŸæŸ¥è¯¢ï¼Œæœ€è¿‘ä½¿ç”¨çš„æœ€å¤š 1024 ä¸ªç»“æœä¼šè¢«ç¼“å­˜ï¼Œé¿å…é¢‘ç¹é‡å¤å‘ KG å‘èµ·ç›¸åŒè¯·æ±‚
@lru_cache(maxsize=1024)
def id2entity_name_or_type(entity_id):
    init_id = entity_id

    # æ„é€  SPARQL æŸ¥è¯¢è¯­å¥
    entity_id = sparql_id % (format(entity_id), format(entity_id))

    # prepare SPARQL query
    sparql = SPARQLWrapper(SPARQLPATH)
    sparql.setQuery(entity_id)
    sparql.setReturnFormat(JSON)

    # send SPARQL query
    results = []
    attempts = 0
    while attempts < 3:  # Set the number of retries
        try:
            results = sparql.query().convert()
            break
            # return results["results"]["bindings"]
        except Exception as e:
            print("404 Error encountered. Retrying after 2 seconds...")
            print(e)
            time.sleep(2)  # Sleep for 2 seconds before retrying
            attempts += 1  

    if attempts == 3:
        print("Failed to execute after multiple attempts.")

    # å¤„ç†æŸ¥è¯¢ç»“æœ
    if len(results["results"]["bindings"]) == 0:    # if no results are found, return "Unnamed Entity"
        return "Unnamed Entity"
    else:
        # ä¼˜å…ˆé€‰æ‹©è‹±æ–‡åï¼ˆxml:lang == 'en'ï¼‰
        english_results = [result['tailEntity']['value'] for result in results["results"]["bindings"] if result['tailEntity'].get('xml:lang') == 'en']
        if english_results:
            return english_results[0]  # Return the first English result

        #   If no English labels are found, checks for names that contain only letters and numbers (ignores symbols)
        alphanumeric_results = [result['tailEntity']['value'] for result in results["results"]["bindings"]
                                if re.match("^[a-zA-Z0-9 ]+$", result['tailEntity']['value'])]
        if alphanumeric_results:
            return alphanumeric_results[0]  # Return the first alphanumeric result
        
        # å¦‚æœæŸ¥è¯¢å¤±è´¥æˆ–ç»“æœä¸åˆæ³•ï¼Œåˆ™è¿”å› "Unnamed Entity"ã€‚
        return "Unnamed Entity"



# ä¸€ç»„å®ä½“å‡ºå‘ï¼Œå¹¶è¡Œåœ°å‘å¤–æ‰©å±•ä¸€è·³é‚»å±…ï¼Œæ›´æ–°å›¾ç»“æ„å’Œå®ä½“ä¿¡æ¯
def explore_graph_from_one_topic_entities(current_entities, graph, entity_names, exlored_entities, all_entities, answer_name=[]):
    """
    current_entities: set(str), å½“å‰è½®æ¬¡ä¸­è¦æ‰©å±•çš„å®ä½“é›†åˆï¼ˆFreebase MIDï¼‰
    graph: dict, å½“å‰æ­£åœ¨æ„å»ºçš„å›¾ç»“æ„
    entity_names: dict, MID â†’ å®ä½“åç§°
    exlored_entities: set(str), å·²æ‰€æœ‰å·²æ¢ç´¢è¿‡çš„å®ä½“ï¼Œé˜²æ­¢é‡å¤
    all_entities: set(str), æ‰€æœ‰å‘ç°çš„å®ä½“ï¼ˆå…¨å±€é›†åˆï¼‰
    answer_name: list(str), æ­£ç¡®ç­”æ¡ˆçš„å®ä½“åç§°
    """
    storage_lock = Lock()  # ä¸ºå¤šçº¿ç¨‹è®¿é—®å…±äº«æ•°æ®ç»“æ„ï¼ˆå¦‚ graphï¼‰è®¾ç½®çš„çº¿ç¨‹é”ã€‚
    new_entities = set()    # å½“å‰è½®æ–°å‘ç°çš„å®ä½“é›†åˆ
    answer_name_set = set(answer_name)
    found_answer_entities = set()
    
    # å¤šçº¿ç¨‹å¹¶å‘ï¼šä¸ºæ¯ä¸ªå®ä½“æäº¤ä¸€ä¸ªå¼‚æ­¥ä»»åŠ¡ï¼ŒæŸ¥è¯¢å®ƒçš„ä¸€è·³è¾¹
    with ThreadPoolExecutor(max_workers=8) as executor:    # åˆ›å»ºçº¿ç¨‹æ± 
        futures = {executor.submit(search_relations_and_entities_combined_1, entity): entity for entity in current_entities}

        exlored_entities.update(current_entities)   # å°† current_entities åŠ å…¥ exlored_entities æ ‡è®°ä¸ºå·²å¤„ç†ã€‚

        # å¤„ç†æ¯ä¸ªå¼‚æ­¥ä»»åŠ¡çš„ç»“æœï¼ˆä¸€ä¸ªå®ä½“çš„æ‰€æœ‰è¾¹ï¼‰
        for future in as_completed(futures):    # as_completed(futures) waits for queries to finish in any order.
            results = future.result()   # get query result
            entity = futures[future]    # retrieve corresponding entity of the finished "future"

            # å¤„ç†æ¯æ¡è¾¹ç»“æœ
            for result in results:      # eg. a single result (one edge in KG) = { "relation": "shares_border_with", "connectedEntity": "m.0f8l9c", "connectedEntityName": "France", "direction": "tail" }
                relation, connected_entity, connected_name, direction = result['relation'], result['connectedEntity'], result['connectedEntityName'], result['direction']

                if connected_entity.startswith("m."):
                    if connected_entity in exlored_entities:    # skip already explored entities
                        continue

                    # æ£€æŸ¥æ˜¯å¦åŒ¹é…åˆ°ç­”æ¡ˆåç§°
                    if connected_name in answer_name_set:
                        found_answer_entities.add(connected_entity)

                    with storage_lock:      # lock when modifying shared structures
                        # æ›´æ–°æˆ–æ·»åŠ å®ä½“åç§°
                        entity_names[connected_entity] = connected_name

                        # ç¡®ä¿å›¾ä¸­åŒ…å«ç›¸å…³å®ä½“å’Œå…³ç³»
                        if entity not in graph:
                            graph[entity] = {}

                        if connected_entity not in graph:
                            graph[connected_entity] = {}

                        if connected_entity not in graph[entity]:
                            graph[entity][connected_entity] = {'forward': set(), 'backward': set()}

                        if entity not in graph[connected_entity]:
                            graph[connected_entity][entity] = {'forward': set(), 'backward': set()}

                        # æ›´æ–°å…³ç³»
                        if direction == "tail":
                            graph[entity][connected_entity]['forward'].add(relation)
                            graph[connected_entity][entity]['backward'].add(relation)
                        else:  # direction is "head"
                            graph[entity][connected_entity]['backward'].add(relation)
                            graph[connected_entity][entity]['forward'].add(relation)

                    new_entities.add(connected_entity)

        # æ›´æ–°æ–°å®ä½“é›†åˆ & ä¸‹ä¸€è½®å‡†å¤‡
        new_entities.difference_update(exlored_entities)    # remove explored entities from new_entities
        all_entities.update(new_entities)                   # add new entities to the global entity set
        current_entities = new_entities                     # set new entities as "current_entities" for next round
        
        # åˆ¤æ–­æ˜¯å¦æ‰¾åˆ°å·¦å³ç­”æ¡ˆ entity
        found_all_answers = len(found_answer_entities) == len(answer_name_set)

    # print("Entities are not fully connected or answer entity not found within the maximum allowed hops.")
    return (graph, all_entities, exlored_entities, current_entities, entity_names, found_all_answers)



# ä»ä¸€ä¸ªå®ä½“å‡ºå‘ï¼Œåœ¨ Freebase çŸ¥è¯†å›¾è°±ä¸­æŸ¥è¯¢ä¸ä¹‹è¿æ¥çš„æ‰€æœ‰ä¸‰å…ƒç»„ï¼Œå¹¶è¿”å›æ¸…æ´—åçš„ç»“æ„åŒ–ç»“æœã€‚
def search_relations_and_entities_combined_1(entity_id):

    # æŸ¥è¯¢æŸä¸ªå®ä½“ä¸å…¶è¿æ¥çš„æ‰€æœ‰è¾¹ï¼ˆå…³ç³»ï¼‰ä¸ç›¸é‚»å®ä½“ï¼Œå¹¶è¿”å›å®ƒä»¬çš„åç§°å’Œè¿æ¥æ–¹å‘ã€‚
    sparql_query = """
    PREFIX ns: <http://rdf.freebase.com/ns/>
    SELECT ?relation ?connectedEntity ?connectedEntityName ?direction
    WHERE {
        {
            ns:%s ?relation ?connectedEntity .
            OPTIONAL {
                ?connectedEntity ns:type.object.name ?name .
                FILTER(lang(?name) = 'en')
            }
            BIND(COALESCE(?name, "Unnamed Entity") AS ?connectedEntityName)
            BIND("tail" AS ?direction)
        }
        UNION
        {
            ?connectedEntity ?relation ns:%s .
            OPTIONAL {
                ?connectedEntity ns:type.object.name ?name .
                FILTER(lang(?name) = 'en')
            }
            BIND(COALESCE(?name, "Unnamed Entity") AS ?connectedEntityName)
            BIND("head" AS ?direction)
        }
    }
    """ % (entity_id, entity_id)

    # Execute the SPARQL Query
    results = execute_sparql(sparql_query)  # sends the query to a SPARQL endpoint, retrieves JSON results.

    return replace_prefix1(results)



# å°† http://rdf.freebase.com/ns/ å‰ç¼€æ¸…æ´—æ‰ï¼Œä¿ç•™ m.02jx3 è¿™æ ·çš„ç®€å†™ MIDã€‚
def replace_prefix1(data):
    if data is None:
        print("Warning: No data available to process in replace_prefix1.")
        return []

    # Returns a list of cleaned dictionaries.
    return [{key: value['value'].replace("http://rdf.freebase.com/ns/", "") for key, value in result.items()} for result in data]



# Sends a SPARQL query to a Freebase/Wikidata endpoint and retrieves structured data in JSON format.
#   Returns a list of dictionaries (extracted from "bindings") or None if the query fails.
def execute_sparql(sparql_txt):

    # Assuming SPARQLPATH is a variable that holds your SPARQL endpoint URL
    sparql = SPARQLWrapper(SPARQLPATH)      # Creates a SPARQL query wrapper
    # sparql.setQuery(sparql_txt)
    # sparql.setQuery("define sql:big-data-const 0\n" + sparql_txt)
    sparql.setQuery("define sql:big-data-const 0\n" + sparql_txt + "\nLIMIT 3000")
    sparql.setReturnFormat(JSON)
    
    attempts = 0
    while attempts < 3:  # Set the number of retries
        try:
            results = sparql.query().convert()
            return results["results"]["bindings"]
        except Exception as e:
            print("404 Error encountered. Retrying after 2 seconds...")
            print(e)

            time.sleep(2)  # Sleep for 2 seconds before retrying
            attempts += 1  

    print("Failed to execute after multiple attempts.")
    return None



# åœ¨æœ€å¤š max_depth è·³å†…ï¼Œä»å¤šä¸ª topic entities åŒæ—¶å‡ºå‘ï¼Œæœç´¢é‚»æ¥å®ä½“ï¼Œå¹¶æ„å»ºä¸€ä¸ªåŒ…å«è¿™äº›å®ä½“ã€å…³ç³»ã€æ–¹å‘çš„å›¾ç»“æ„ï¼Œ
# ç›´åˆ°æ‰¾åˆ°æ‰€æœ‰ç­”æ¡ˆå®ä½“å¹¶åˆ¤æ–­è¿™äº› topic entities æ˜¯å¦äº’ç›¸è¿é€šã€‚
# entity_ids: å¤šä¸ª topic entity çš„ Freebase IDï¼ˆä¾‹å¦‚ "m.02jx3"ï¼‰
# answer_name: æ­£ç¡®ç­”æ¡ˆçš„åå­—åˆ—è¡¨ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦æ‰¾åˆ°ç­”æ¡ˆï¼‰
def explore_graph_from_entities_by_hop_neighbor_1(entity_ids, max_depth=5, answer_name=[]):
    current_entities = set(entity_ids)  # å½“å‰è¿™ä¸€å±‚çš„æ¢ç´¢èŠ‚ç‚¹
    all_entities = set(entity_ids)      # æ€»å…±è§è¿‡çš„èŠ‚ç‚¹
    found_answer = False                # æ˜¯å¦æ‰¾åˆ°æ‰€æœ‰ç­”æ¡ˆ

    entity_names = {entity: id2entity_name_or_type(entity) for entity in entity_ids}  # é»˜è®¤æ‰€æœ‰åˆå§‹å®ä½“åç§°ä¸º"unnamedentity"
    graph = {entity: {} for entity in all_entities}  # åˆå§‹åŒ–å›¾

    storage_lock = Lock()               # åˆ›å»ºçº¿ç¨‹å®‰å…¨é”
    
    answer_name_set = set(answer_name) 
    empty_set = set()                   # ç”¨äºåˆ¤æ–­å·²æ‰¾åˆ°çš„ç­”æ¡ˆå®ä½“
    if len(entity_ids) == 1:
        connect = True
    else:
        connect = False
    hopnumber = 5

    #  å¤šè½® BFSï¼šä»å¤šä¸ªå®ä½“ä¸€èµ·å‡ºå‘ï¼Œæ¢ç´¢é‚»å±…ç›´åˆ° max_depth
    for depth in range(1, max_depth + 1):
        print(f"Exploring entities at depth {depth}...")
        start = time.time()
        new_entities = set()

        # ç”¨å¤šçº¿ç¨‹å¹¶å‘æœç´¢é‚»å±…å®ä½“
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = {executor.submit(search_relations_and_entities_combined_1, entity): entity for entity in current_entities}

            # å¯¹è¿”å›çš„ç»“æœå¤„ç†å›¾ç»“æ„ + ç­›é€‰ç­”æ¡ˆ
            for future in as_completed(futures):
                results = future.result()
                entity = futures[future]
                for result in results:
                    relation, connected_entity, connected_name, direction = result['relation'], result['connectedEntity'], result['connectedEntityName'], result['direction']

                    if connected_entity.startswith("m."):
                        # æ£€æŸ¥ connected_name æ˜¯å¦åœ¨ç­”æ¡ˆé›†ä¸­ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ ‡è®°ä¸ºç­”æ¡ˆå®ä½“
                        if connected_name in answer_name_set:
                            empty_set.add(connected_entity)
                            if len(empty_set) == len(answer_name_set):
                                found_answer = True
                        # æ›´æ–°å›¾
                        with storage_lock:
                            # æ›´æ–°æˆ–æ·»åŠ å®ä½“åç§°
                            entity_names[connected_entity] = connected_name
                            # ç¡®ä¿å›¾ä¸­åŒ…å«ç›¸å…³å®ä½“å’Œå…³ç³»
                            if entity not in graph:
                                graph[entity] = {}
                            if connected_entity not in graph:
                                graph[connected_entity] = {}
                            if connected_entity not in graph[entity]:
                                graph[entity][connected_entity] = {'forward': set(), 'backward': set()}
                            if entity not in graph[connected_entity]:
                                graph[connected_entity][entity] = {'forward': set(), 'backward': set()}

                            # æ›´æ–°å…³ç³»
                            if direction == "tail":
                                graph[entity][connected_entity]['forward'].add(relation)
                                graph[connected_entity][entity]['backward'].add(relation)
                            else:  # direction is "head"
                                graph[entity][connected_entity]['backward'].add(relation)
                                graph[connected_entity][entity]['forward'].add(relation)
                        new_entities.add(connected_entity)


        new_entities.difference_update(all_entities)
        all_entities.update(new_entities)
        current_entities = new_entities
        end = time.time()
        print(f"Time taken to explore depth {depth}: {end - start:.2f} seconds")
        if connect == False:
            connect = are_entities_connected(graph, entity_ids, all_entities)
            if connect:
                print(f"All entities are connected within {depth} hops.")
                hopnumber = depth

        if found_answer and connect:
            return (found_answer, graph, hopnumber, all_entities, current_entities, entity_names, connect)

    print("Entities are not fully connected or answer entity not found within the maximum allowed hops.")
    return (found_answer, graph, hopnumber, all_entities, current_entities, entity_names, connect)



# æ‰€æœ‰ topic entities æ˜¯å¦å¤„äºåŒä¸€ä¸ªè¿é€šå­å›¾ä¸­
def are_entities_connected(graph, total_entities, all_entities):
    """
    Check if starting from the first entity in total_entities, all other entities in total_entities can be visited.
    graph: Dictionary with entity as key and another dictionary {connected_entity: {'forward': set(), 'backward': set()}} as value.
    total_entities: Set of initial entities to check connectivity from.
    """
    # å¦‚æœæ²¡æœ‰å®ä½“ï¼Œåˆ™è¿é€š
    if not total_entities:
        return True

    total_entities_set = set(total_entities)

    # å¦‚æœåªæœ‰ä¸€ä¸ªå®ä½“ï¼Œåˆ™è”é€š
    if len(total_entities_set) == 1:
        return True

    # ä»ä»»æ„ä¸€ä¸ªå®ä½“å‡ºå‘åš BFS
    start_entity = next(iter(total_entities_set))
    visited = set()
    queue = deque([start_entity])

    while queue:
        current = queue.popleft()
        if current not in visited:
            visited.add(current)
            # Early termination check
            if total_entities_set.issubset(visited):
                return True

            # Add connected entities to the queue
            for connected_entity, relations in graph[current].items():
                if connected_entity not in visited:
                    queue.append(connected_entity)

    # Final check in case not all entities are connected
    return False



# def retrieve_answer_from_kg_simpleqa(topic_mid, relation_keyword, expected_answer_text):
#     sparql = SPARQLWrapper(SPARQL_ENDPOINT)

#     # Step 1: è·å– topic_mid æ‰€æœ‰è°“è¯
#     query1 = f"""
#     PREFIX ns: <http://rdf.freebase.com/ns/>
#     SELECT DISTINCT ?p
#     FROM <http://freebase.com>
#     WHERE {{
#         ns:{topic_mid} ?p ?o .
#     }}
#     LIMIT 100
#     """
#     sparql.setQuery(query1)
#     sparql.setReturnFormat(JSON)
#     try:
#         results = sparql.query().convert()
#         predicates = [
#             b["p"]["value"].replace("http://rdf.freebase.com/ns/", "")
#             for b in results["results"]["bindings"]
#         ]
#     except Exception:
#         return None, None, None, [], {}

#     # Step 2: æ¨¡ç³ŠåŒ¹é… relation keyword
#     matched_preds = [p for p in predicates if relation_keyword.lower() in p.lower().split(".")[-1]]
#     tried_answers = {}

#     if not matched_preds:
#         return None, None, None, matched_preds, tried_answers

#     # Step 3: éå†è°“è¯ï¼Œæ‰¾å‡ºç¬¦åˆ expected_answer_text çš„ entity
#     for predicate in matched_preds:
#         query2 = f"""
#         PREFIX ns: <http://rdf.freebase.com/ns/>
#         SELECT DISTINCT ?mid ?name
#         FROM <http://freebase.com>
#         WHERE {{
#             ns:{topic_mid} ns:{predicate} ?mid .
#             OPTIONAL {{ ?mid ns:type.object.name ?name . FILTER (lang(?name) = "en") }}
#         }}
#         LIMIT 200
#         """
#         sparql.setQuery(query2)
#         sparql.setReturnFormat(JSON)

#         try:
#             res = sparql.query().convert()
#             answer_list = []
#             for b in res["results"]["bindings"]:
#                 mid = b["mid"]["value"].replace("http://rdf.freebase.com/ns/", "")
#                 label = b["name"]["value"].strip() if "name" in b else None
#                 answer_list.append((mid, label))

#                 if label and label.lower() == expected_answer_text.lower():
#                     return mid, label, predicate, matched_preds, tried_answers

#             tried_answers[predicate] = answer_list

#         except Exception:
#             continue

#     # Step 4: fallbackï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªéç©º relation çš„ç¬¬ä¸€ä¸ª entity
#     for pred, items in tried_answers.items():
#         for mid, label in items:
#             if label:  # âœ… åªè¿”å› label éç©ºçš„
#                 return mid, label, pred, matched_preds, tried_answers

#     return None, None, None, matched_preds, tried_answers



def construct_subgraph(dataset, data, topic_entity, entity_names, question_real_answer, depth):
    print("--------- Construct Subgraph ---------")
    explored_entities = set()                           # å·²ç»è¢«è®¿é—®è¿‡çš„å®ä½“
    next_explore_entities = set(topic_entity.keys())    # å½“å‰è¿™è½® BFS å°†è¦æ‰©å±•çš„å®ä½“ï¼ˆåˆå§‹ä¸º topic entitiesï¼‰
    all_entities = set(topic_entity.keys())             # è®°å½•å›¾ä¸­æ‰€æœ‰è®¿é—®åˆ°çš„å®ä½“

    # ä¸´æ—¶ç»´æŠ¤çš„å­å›¾ï¼ˆé‚»æ¥è¡¨ç»“æ„ï¼‰
    graph = {entity: {} for entity in topic_entity.keys()}

    # ------------------------- SimpleQA -------------------------
    if dataset == "simpleqa":
        # ...
        pass

    # ------------------------- WebQuestions -------------------------
    elif dataset == "webquestions":
        # ...
        pass

    # ------------------------- GrailQA -------------------------
    elif dataset == "grailqa":
        # ...
        pass

    # ------------------------- cwq -------------------------
    elif dataset == "cwq":
        sparql = data.get("sparql", "")
        if not sparql:
            print("âš ï¸ æ—  SPARQL æŸ¥è¯¢è¯­å¥ï¼Œè·³è¿‡")
            return graph, entity_names

        # è·å–æ‰€æœ‰å®ä½“ä¸‰å…ƒç»„ï¼ˆä¸åŒ…å« FILTER/PREFIXï¼‰
        triple_lines = [
            line.strip() for line in sparql.split('\n')
            if 'ns:' in line and not line.strip().startswith(('PREFIX', 'FILTER', '#'))
        ]

        # æå–æ‰€æœ‰ mid
        mids_in_triples = extract_mids(triple_lines)

        # è§£æ mid çš„ label
        label_map = resolve_entity_labels(mids_in_triples)

        # æ„é€ å›¾ç»“æ„
        for line in triple_lines:
            parts = re.findall(r'ns:([^\s]+)', line)
            if len(parts) == 3:
                h_raw, rel_raw, t_raw = parts
                h = h_raw if h_raw.startswith("m.") else f"?{h_raw}"
                t = t_raw if t_raw.startswith("m.") else f"?{t_raw}"
                rel = rel_raw

                # å¦‚æœæ˜¯å®ä½“ï¼ˆmidï¼‰ï¼Œåˆ™åˆå§‹åŒ–å›¾èŠ‚ç‚¹
                for node in [h, t]:
                    if node.startswith("m."):
                        if node not in graph:
                            graph[node] = {}
                        all_entities.add(node)
                        if node not in entity_names:
                            entity_names[node] = label_map.get(node, "UNKNOWN")

                # å»ºå›¾ï¼ˆä»…å®ä½“å¯¹ï¼‰
                if h.startswith("m.") and t.startswith("m."):
                    if t not in graph[h]:
                        graph[h][t] = {'forward': set(), 'backward': set()}
                    if h not in graph[t]:
                        graph[t][h] = {'forward': set(), 'backward': set()}
                    graph[h][t]['forward'].add(rel)
                    graph[t][h]['backward'].add(rel)
                    print(f"â¤ {h} ({entity_names[h]}) --{rel}--> {t} ({entity_names[t]})")

        # å°†ç­”æ¡ˆå®ä½“åŠ å…¥ explored/all/entities
        for ans_mid in question_real_answer:
            entity_names[ans_mid] = get_label_for_mids([ans_mid]).get(ans_mid, "UNKNOWN")
            all_entities.add(ans_mid)
            explored_entities.add(ans_mid)
            if ans_mid not in graph:
                graph[ans_mid] = {}

        # è®°å½•æ‰€æœ‰ topic_entity
        for mid in topic_entity:
            explored_entities.add(mid)

        print(f"âœ… æ„å›¾å®Œæˆï¼Œå…±å®ä½“: {len(all_entities)}, è¾¹æ•°: {sum(len(v) for v in graph.values())}")
