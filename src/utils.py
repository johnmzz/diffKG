import os
import torch
import difflib
from os.path import basename, isfile
from os import makedirs
from glob import glob
import networkx as nx
import pickle
import json
from texttable import Texttable
from functools import lru_cache
import re
from collections import deque, defaultdict
from SPARQLWrapper import SPARQLWrapper, JSON, XML
from openai import OpenAI
import time
import sqlite3
from urllib.error import HTTPError
from threading import Lock
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig


SPARQLPATH = "http://localhost:9890/sparql"  # depend on your own internal address and port, shown in Freebase folder's readme.md


sparql_head_relations = """\nPREFIX ns: <http://rdf.freebase.com/ns/>\nSELECT ?relation\nWHERE {\n  ns:%s ?relation ?x .\n}"""
sparql_tail_relations = """\nPREFIX ns: <http://rdf.freebase.com/ns/>\nSELECT ?relation\nWHERE {\n  ?x ?relation ns:%s .\n}"""
sparql_tail_entities_extract = """PREFIX ns: <http://rdf.freebase.com/ns/>\nSELECT ?tailEntity\nWHERE {\nns:%s ns:%s ?tailEntity .\n}""" 
sparql_head_entities_extract = """PREFIX ns: <http://rdf.freebase.com/ns/>\nSELECT ?tailEntity\nWHERE {\n?tailEntity ns:%s ns:%s  .\n}"""
sparql_id = """PREFIX ns: <http://rdf.freebase.com/ns/>\nSELECT DISTINCT ?tailEntity\nWHERE {\n  {\n    ?entity ns:type.object.name ?tailEntity .\n    FILTER(?entity = ns:%s)\n  }\n  UNION\n  {\n    ?entity <http://www.w3.org/2002/07/owl#sameAs> ?tailEntity .\n    FILTER(?entity = ns:%s)\n  }\n}"""


sparql_tail_entities_and_relations = """
PREFIX ns: <http://rdf.freebase.com/ns/>
SELECT ?relation ?tailEntity
WHERE {
    ns:%s ?relation ?tailEntity .
}
"""

sparql_head_entities_and_relations = """
PREFIX ns: <http://rdf.freebase.com/ns/>
SELECT ?relation ?headEntity
WHERE {
    ?headEntity ?relation ns:%s .
}
"""



def tab_printer(args):
    """
    Function to print the logs in a nice tabular format.
    :param args: Parameters used for the model.
    """
    args = vars(args)
    keys = sorted(args.keys())
    t = Texttable()
    rows = [["Parameter", "Value"]] + [[k.replace("_", " ").capitalize(), args[k]] for k in keys]
    t.add_rows(rows)
    print(t.draw())



def initialize_large_database(db_path):
    # connect to the SQLite database file specified by db_path
    with sqlite3.connect(db_path) as conn:
        cursor = conn.cursor()  # create cursor, which allows SQL commands to be withini the DB

        # create a table named subgraphs
        #   question_id: stores the ID of the question being processed
        #   chunk_index: chunk number for large subgraph data is split into multiple pieces
        #   data: stores the actual subgraph information in binary format
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS subgraphs (
            question_id TEXT,
            chunk_index INTEGER,
            data BLOB,
            PRIMARY KEY (question_id, chunk_index)
        )
        ''')
        conn.commit()   # save the changes to the SQL DB



def retry_operation(func):
    """Decorator to retry database operations."""
    def wrapper(*args, **kwargs):
        attempts = 0
        max_retries = kwargs.pop('max_retries', 300)
        wait_time = kwargs.pop('wait_time', 6)
        while attempts < max_retries:
            try:
                return func(*args, **kwargs)
            except sqlite3.OperationalError as e:
                if 'database is locked' in str(e):
                    print(f"Database is locked, retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                    attempts += 1
                else:
                    print("An error occurred:", e)
                    break
        print("Failed after several attempts.")
        return None
    return wrapper

@retry_operation
def delete_data_by_question_id(db_path, question_id):
    with sqlite3.connect(db_path) as conn:
        start = time.time()
        cursor = conn.cursor()
        cursor.execute('DELETE FROM subgraphs WHERE question_id = ?', (question_id,))
        conn.commit()
        # print(f"Data associated with question_id {question_id} has been deleted.")
        # print(f"Time taken to delete data: {time.time() - start} seconds.")

@retry_operation
def save_to_large_db(db_path, question_id, data_dict, chunk_size=256 * 1024 * 1024):
    with sqlite3.connect(db_path) as conn:
        start = time.time()
        cursor = conn.cursor()
        data_blob = pickle.dumps(data_dict)
        for i in range(0, len(data_blob), chunk_size):
            chunk = data_blob[i:i+chunk_size]
            cursor.execute('INSERT INTO subgraphs (question_id, data, chunk_index) VALUES (?, ?, ?)',
                           (question_id, chunk, i // chunk_size))
            conn.commit()
        # print("Data saved to database.")
        # print(f"Time taken to save data: {time.time() - start} seconds.")

# Retrieves and reconstructs stored data from an SQLite database based on a question_id
@retry_operation
def load_from_large_db(db_path, question_id):
    with sqlite3.connect(db_path) as conn:  # connect with DB
        start = time.time()
        cursor = conn.cursor()

        # Retrieves the "data" column of records where "question_id" matches
        cursor.execute('SELECT data FROM subgraphs WHERE question_id = ? ORDER BY chunk_index', (question_id,))
        
        # Concatenates all retrieved data values into a single binary blob
        data_blob = b''.join(row[0] for row in cursor if row[0] is not None)
        
        if not data_blob:
            # print("No data found or data is empty.")
            return None
        
        # Deserializes (unpickles) the binary blob back into its original Python object.
        try:
            result = pickle.loads(data_blob)
            return result
        except EOFError as e:
            print(f"EOFError when unpickling data: {e}")
            return None


def query_label_from_kg(mid, sparql_endpoint="http://localhost:9890/sparql"):
    sparql = SPARQLWrapper(sparql_endpoint)
    query = f"""
    PREFIX ns: <http://rdf.freebase.com/ns/>
    SELECT ?name
    FROM <http://freebase.com>
    WHERE {{
        ns:{mid} ns:type.object.name ?name .
        FILTER (lang(?name) = "en")
    }}
    """
    sparql.setQuery(query)
    sparql.setReturnFormat(JSON)
    try:
        result = sparql.query().convert()
        bindings = result.get("results", {}).get("bindings", [])
        return bindings[0]["name"]["value"] if bindings else None
    except:
        return None



def prepare_dataset(dataset_name):
    if dataset_name == 'cwq':
        with open('../data/cwq.json',encoding='utf-8') as f:
            datas = json.load(f)
        question_string = 'question'
        ID = 'ID'

    elif dataset_name == 'cwq_multi':
        with open('../data/cwq_multi.json',encoding='utf-8') as f:
            datas = json.load(f)
        ID = 'ID'
        question_string = 'question'

    elif dataset_name == 'webqsp':
        with open('../data/WebQSP.json',encoding='utf-8') as f:
            datas = json.load(f)
        question_string = 'RawQuestion'
        ID = "QuestionId"
        # answer = ""

    elif dataset_name == 'webqsp_multi':
        with open('../data/webqsp_multi.json',encoding='utf-8') as f:
            datas = json.load(f)
        question_string = 'question'

    elif dataset_name == 'grailqa':
        with open('../data/grailqa.json',encoding='utf-8') as f:
            datas = json.load(f)
        question_string = 'question'
        ID = 'qid'
        
    elif dataset_name == 'simpleqa':
        with open('../data/SimpleQA.json',encoding='utf-8') as f:
            datas = json.load(f)    
        question_string = 'question'
        ID = 'question'
        answer_string = 'answer'

    elif dataset_name == 'webquestions':
        with open('../data/WebQuestions.json',encoding='utf-8') as f:
            datas = json.load(f)
        ID = 'question'
        question_string = 'question'

    else:
        print("dataset not found, you should pick from {cwq, webqsp, grailqa, simpleqa, qald, webquestions, trex, zeroshotre, creak}.")
        exit(-1)
    return datas, question_string, ID



def check_answerlist(dataset_name, origin_data, topic_entity):
    final_answer_list = []
    answer_list = []
    # origin_data = [j for j in ground_truth_datas if j[question_string] == ori_question]
    if dataset_name == 'cwq':
        for k, v in origin_data["answer"].items():
            answer_list.append(
                {"mid": k, "label": v}
            )

    elif dataset_name == 'webqsp':
        answers = origin_data["Parses"]
        for parse in answers:
            for ans in parse['Answers']:
                if ans.get("AnswerType") == "Entity":
                    mid = ans.get("AnswerArgument")
                    label = query_label_from_kg(mid)
                    if label:
                        answer_list.append(
                            {"mid": mid, "label": label}
                        )

    elif dataset_name == 'grailqa':
        answers = origin_data["answer"]
        for answer in answers:
            if answer.get("answer_type") == "Entity":
                mid = answer["answer_argument"]
                label = query_label_from_kg(mid)
                if label:
                    answer_list.append(
                        {"mid": mid, "label": label}
                    )

    elif dataset_name == 'simpleqa':
        for k,v in origin_data["real_answer"].items():
            answer_list.append(
                {"mid": k, "label": v, "relation": origin_data["relation"]}
            )

    elif dataset_name == 'webquestions':
        for k,v in origin_data["real_answers"].items():
            answer_list.append(
                {"mid": k, "label": v["label"], "relation": v["relation"]}
            )

    return [dict(t) for t in {tuple(sorted(d.items())) for d in answer_list}]



def initialize_large_database(db_path):
    # connect to the SQLite database file specified by db_path
    with sqlite3.connect(db_path) as conn:
        cursor = conn.cursor()  # create cursor, which allows SQL commands to be withini the DB

        # create a table named subgraphs
        #   question_id: stores the ID of the question being processed
        #   chunk_index: chunk number for large subgraph data is split into multiple pieces
        #   data: stores the actual subgraph information in binary format
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS subgraphs (
            question_id TEXT,
            chunk_index INTEGER,
            data BLOB,
            PRIMARY KEY (question_id, chunk_index)
        )
        ''')
        conn.commit()   # save the changes to the SQL DB



# model_id: Huggingface æ¨¡å‹çš„å­—ç¬¦ä¸²è·¯å¾„ï¼Œä¾‹å¦‚ "openchat/openchat-3.5-0106"
# cache_dir: æœ¬åœ°æ¨¡å‹ç¼“å­˜ç›®å½•ï¼Œå°†æ¨¡å‹ä¸‹è½½å¹¶ä¿å­˜åœ¨è¿™é‡Œ
def init_LLM(model_id: str, cache_dir: str = "/data1/zhuom/LLM"):
    """
    åŠ è½½æŒ‡å®šæ¨¡å‹ï¼Œè‡ªåŠ¨é€‚é… prompt å¹¶æ‰§è¡Œæµ‹è¯•ã€‚
    æ”¯æŒå¤šä¸ª Huggingface æœ¬åœ°æ¨¡å‹ã€‚
    """
    print(f"ğŸ“¦ å‡†å¤‡åŠ è½½æ¨¡å‹: {model_id}")

    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        cache_dir=cache_dir,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # ========= Prompt æ„é€ å™¨ =========
    def format_prompt(user_input: str) -> str:
        # ChatML æ ·å¼ï¼ˆOpenChat, Mistral, Nous, Mixtral, Yi, Qwen ç­‰ï¼‰
        if any(key in model_id.lower() for key in [
            "openchat", "mistral", "mixtral", "nous", "zephyr", "starling", "yi", "qwen", "gemma"
        ]):
            return (
                "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
                f"<|im_start|>user\n{user_input}<|im_end|>\n"
                "<|im_start|>assistant\n"
            )
        # LLaMA 2/3 ç³»åˆ— INST æ ¼å¼
        elif "llama" in model_id.lower():
            return f"[INST] <<SYS>>\nYou are a helpful assistant.\n<</SYS>>\n\n{user_input} [/INST]"
        else:
            raise NotImplementedError(f"âŒ æœªæ”¯æŒè¯¥æ¨¡å‹çš„ prompt æ ¼å¼: {model_id}")

    # ========= è‡ªåŠ¨é€‰æ‹©æµ‹è¯•é—®é¢˜ =========
    test_question = {
        "openchat/openchat-3.5-0106": "What is the capital of France?",
        "mistralai/Mistral-7B-Instruct-v0.3": "What is the capital of China?",
        "NousResearch/Nous-Hermes-2-Mistral-7B-DPO": "What is the capital of Japan?",
        "mistralai/Mixtral-8x7B-Instruct-v0.1": "What are the main causes of climate change?",
        "meta-llama/Llama-2-13b-chat-hf": "Who wrote the novel '1984'?",
        "meta-llama/Meta-Llama-3-8B-Instruct": "Explain how a neural network learns.",
        "google/gemma-7b-it": "What is the boiling point of water?",
        "HuggingFaceH4/zephyr-7b-beta": "How does photosynthesis work?",
        "berkeley-nest/Starling-LM-7B-alpha": "What is the tallest mountain on Earth?",
        "Qwen/Qwen1.5-7B-Chat": "How many continents are there?",
        "01-ai/Yi-1.5-9B-Chat": "What is the Pythagorean theorem?"
    }.get(model_id, "What is the capital of France?")  # é»˜è®¤ fallback

    prompt = format_prompt(test_question)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # è®¾ç½®ç”Ÿæˆé…ç½®ï¼ˆé¿å… warningï¼‰
    gen_config = GenerationConfig.from_model_config(model.config)
    gen_config.do_sample = True
    gen_config.temperature = 0.4
    gen_config.top_p = 0.9
    gen_config.max_new_tokens = 128
    gen_config.pad_token_id = tokenizer.eos_token_id

    print(f"ğŸ§  æµ‹è¯•é—®é¢˜: {test_question}")
    with torch.no_grad():
        outputs = model.generate(**inputs, generation_config=gen_config)

    # è§£ç å¹¶åªä¿ç•™ç¬¬ä¸€è½® assistant å›ç­”
    response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    if "<|im_end|>" in response:
        response = response.split("<|im_end|>")[0]
    elif "[/INST]" in prompt and "###" in response:
        response = response.split("###")[0]
    print("\nğŸ’¬ æ¨¡å‹å›å¤ï¼š")
    print(response.strip())

    return model, tokenizer



def run_LLM(prompt, model_name, model=None, tokenizer=None, temperature=0.4):
    result = ''

    # ========== OpenAI GPT æ¨¡å‹ ==========
    if "gpt" in model_name.lower():             # openai_api_base = "http://localhost:8000/v1"
        openai_api_key = "your_api_keys"
        if model_name == "gpt4":
            model_engine = "gpt-4-turbo"
        else:
            model_engine = "gpt-3.5-turbo-0125"

        client = OpenAI(api_key=openai_api_key)     # create an OpenAI client for sending request

        # æ„é€  OpenAI çš„ Chat è¯·æ±‚æ ¼å¼ï¼ˆmessages = [system msg + user prompt]ï¼‰
        system_message = "You are an AI assistant that helps people find information."
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": prompt}
        ]

        # ç”¨æ¥å£å‘èµ·å¯¹è¯ï¼Œæœ€å¤šå°è¯• 3 æ¬¡ï¼Œæ¯æ¬¡å¤±è´¥é—´éš” 2 ç§’ã€‚
        try_time = 0
        while try_time < 3:
            try:
                response = client.chat.completions.create(
                    model=model_engine,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=512,
                    frequency_penalty=0,
                    presence_penalty=0
                )
                result = response.choices[0].message.content
                break
            except Exception as e:
                print(f"OpenAI error: {e}")
                print("Retrying in 2 seconds...")
                try_time += 1
                time.sleep(2)
                
    # ========== æœ¬åœ°æ¨¡å‹ ==========
    else:
        if model is None or tokenizer is None:
            raise ValueError("æœ¬åœ°æ¨¡å‹è°ƒç”¨æ—¶å¿…é¡»ä¼ å…¥ model å’Œ tokenizer")

        # === Prompt åŒ…è£…ï¼šæ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ ¼å¼ ===
        model_name_lower = model_name.lower()

        if any(key in model_name_lower for key in [
            "openchat", "mistral", "mixtral", "nous", "zephyr", "starling", "yi", "qwen", "gemma"
        ]):
            prompt = (
                "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
                f"<|im_start|>user\n{prompt}<|im_end|>\n"
                "<|im_start|>assistant\n"
            )
        elif "llama" in model_name_lower:
            # LLaMA 2/3 æ ¼å¼
            prompt = f"[INST] <<SYS>>\nYou are a helpful assistant.\n<</SYS>>\n\n{prompt} [/INST]"
        else:
            raise NotImplementedError(f"âŒ æœªæ”¯æŒçš„æœ¬åœ°æ¨¡å‹ prompt æ ¼å¼: {model_name}")

        # Tokenize
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        # Generate config
        gen_config = GenerationConfig.from_model_config(model.config)
        gen_config.do_sample = True
        gen_config.temperature = temperature
        gen_config.top_p = 0.9
        gen_config.max_new_tokens = 512
        gen_config.pad_token_id = tokenizer.eos_token_id

        # Generate
        with torch.no_grad():
            outputs = model.generate(**inputs, generation_config=gen_config)

        # Decode output
        decoded = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)

        # æˆªæ–­æ¨¡å‹å›å¤ï¼Œåªä¿ç•™ç¬¬ä¸€è½®
        if "<|im_end|>" in decoded:
            result = decoded.split("<|im_end|>")[0].strip()
        elif "[/INST]" in prompt and "###" in decoded:
            result = decoded.split("###")[0].strip()
        else:
            result = decoded.strip()

    print(f"\n{model_name} response:\n{result}\n")
    return result



# ä» LLM è¿”å›çš„æ–‡æœ¬ï¼ˆåŒ…å«æ¨ç†é“¾ CoTï¼‰ä¸­æå–:
# - æ¨ç†è·¯å¾„é•¿åº¦ï¼ˆpath_lengthï¼‰ï¼šç”¨äºé¢„æµ‹å›¾æ¢ç´¢èµ·å§‹æ·±åº¦ D_predict
# - æ¨ç†é“¾æ–‡æœ¬ï¼ˆthinking_cot_lineï¼‰ï¼šç”¨äºæŒ‡å¯¼è·¯å¾„æ’åºã€å®ä½“æ’åºç­‰ã€‚
def extract_path_length_from_text(text):
    back_up = text

    # åŸºäº " - " æ‹†åˆ†å…³ç³»é“¾ï¼ˆå®ä½“-å…³ç³»-å®ä½“...ï¼‰
    tokens = re.split(r'\s*-\s*', text.strip())

    # è®¡ç®—è·¯å¾„é•¿åº¦
    # æ¯æ¡è·¯å¾„ç»“æ„æ˜¯ï¼šEntity - Relation - Entityï¼Œæ‰€ä»¥ æ¯è·³éœ€è¦ä¸¤ä¸ª tokenï¼ˆä¸€ä¸ªå…³ç³»å’Œä¸€ä¸ªç›®æ ‡å®ä½“ï¼‰
    path_length = (len(tokens) - 1) // 2

    # eæŠ½å–æ¨ç†é“¾ CoT è¡Œ
    match = re.search(r'cot\s*:\s*(.*)', back_up, re.IGNORECASE)

    # è¾“å‡ºç»“æœ
    if match:
        thinking_cot_line = match.group(1).strip()
        # print('æå–çš„æ–‡æœ¬æ˜¯ï¼š')
        # print(thinking_cot_line)
    else:
        print('cannot find the cot line')

    return path_length, thinking_cot_line


# ä» LLM çš„è¿”å›æ–‡æœ¬ä¸­æå– æ‹†åˆ†åçš„å­é—®é¢˜ï¼ˆsplit questionsï¼‰
# Return: ["question1", "question2", ...]
def extract_split_questions(text):
    lines = text.strip().split('\n')
    questions = []  # store extracted split questions

    for line in lines:
        line_no_spaces = line.replace(' ', '')

        # å¦‚æœè¯¥è¡ŒåŒ…å« splitï¼Œæå–å†’å·åé¢çš„å­é—®é¢˜
        if re.search(r'split', line_no_spaces, re.IGNORECASE):
            parts = line.split(':', 1)
            if len(parts) > 1:
                question = parts[1].strip()
                questions.append(question)
            else:
                # å¦‚æœæ²¡æœ‰ ':'ï¼Œæ•´ä¸ªè¡Œä½œä¸ºé—®é¢˜
                questions.append(line.strip())

    return questions


# æ ¹æ® LLM ç”Ÿæˆçš„æ¨ç†é“¾ï¼ˆCoT reasoning lineï¼‰ï¼Œå¯¹ topic entities è¿›è¡Œé¡ºåºé‡æ’ï¼Œä»¥åŒ¹é… LLM çš„æ€ç»´é¡ºåºã€‚
def reorder_entities(cot_line, topic_entity_dict):
    entity_positions = []   # ç”¨äºå­˜å‚¨ (position, entity_name) å…ƒç»„ã€‚

    for entity in topic_entity_dict:
        score, position = find_best_matching_substring(entity, cot_line)

        # Assign a high position if no match is found
        if position != -1:
            entity_positions.append((position, entity))
        else:
            entity_positions.append((float('inf'), entity)) # assign (inf, entity) if not found

    # Sort entities based on their positions in cot_line
    entity_positions.sort()

    sorted_entities = [entity for position, entity in entity_positions]
    return sorted_entities


# Finds the best matching substring of an entity within a given CoT-line.
# Returning: the best match (highest similarity score and starting position)
def find_best_matching_substring(entity, cot_line):
    len_entity = len(entity)
    len_cot = len(cot_line)

    # Consider substrings within reasonable lengths
    min_len = max(1, len_entity // 2)
    max_len = min(len_cot, len_entity * 2)

    best_score = 0
    best_start = -1

    for length in range(min_len, max_len + 1):      # iterate over all string length
        for start in range(len_cot - length + 1):   # sliding window over 
            substring = cot_line[start:start + length]
            score = difflib.SequenceMatcher(None, entity, substring).ratio()    # compute similarity ratio
            if score > best_score:
                best_score = score
                best_start = start

    return best_score, best_start



# ç»™å®šå®ä½“ IDï¼ˆå¦‚ "m.02jx3"ï¼‰ï¼Œé€šè¿‡ SPARQL æŸ¥è¯¢çŸ¥è¯†å›¾è°±ï¼Œè¿”å›è¿™ä¸ªå®ä½“çš„åç§°ï¼ˆä¾‹å¦‚ "Barack Obama"ï¼‰
# ä½¿ç”¨ç¼“å­˜åŠ é€ŸæŸ¥è¯¢ï¼Œæœ€è¿‘ä½¿ç”¨çš„æœ€å¤š 1024 ä¸ªç»“æœä¼šè¢«ç¼“å­˜ï¼Œé¿å…é¢‘ç¹é‡å¤å‘ KG å‘èµ·ç›¸åŒè¯·æ±‚
@lru_cache(maxsize=1024)
def id2entity_name_or_type(entity_id):
    init_id = entity_id

    # æ„é€  SPARQL æŸ¥è¯¢è¯­å¥
    entity_id = sparql_id % (format(entity_id), format(entity_id))

    # prepare SPARQL query
    sparql = SPARQLWrapper(SPARQLPATH)
    sparql.setQuery(entity_id)
    sparql.setReturnFormat(JSON)

    # send SPARQL query
    results = []
    attempts = 0
    while attempts < 3:  # Set the number of retries
        try:
            results = sparql.query().convert()
            break
            # return results["results"]["bindings"]
        except Exception as e:
            print("404 Error encountered. Retrying after 2 seconds...")
            print(e)
            time.sleep(2)  # Sleep for 2 seconds before retrying
            attempts += 1  

    if attempts == 3:
        print("Failed to execute after multiple attempts.")

    # å¤„ç†æŸ¥è¯¢ç»“æœ
    if len(results["results"]["bindings"]) == 0:    # if no results are found, return "Unnamed Entity"
        return "Unnamed Entity"
    else:
        # ä¼˜å…ˆé€‰æ‹©è‹±æ–‡åï¼ˆxml:lang == 'en'ï¼‰
        english_results = [result['tailEntity']['value'] for result in results["results"]["bindings"] if result['tailEntity'].get('xml:lang') == 'en']
        if english_results:
            return english_results[0]  # Return the first English result

        #   If no English labels are found, checks for names that contain only letters and numbers (ignores symbols)
        alphanumeric_results = [result['tailEntity']['value'] for result in results["results"]["bindings"]
                                if re.match("^[a-zA-Z0-9 ]+$", result['tailEntity']['value'])]
        if alphanumeric_results:
            return alphanumeric_results[0]  # Return the first alphanumeric result
        
        # å¦‚æœæŸ¥è¯¢å¤±è´¥æˆ–ç»“æœä¸åˆæ³•ï¼Œåˆ™è¿”å› "Unnamed Entity"ã€‚
        return "Unnamed Entity"



# ä¸€ç»„å®ä½“å‡ºå‘ï¼Œå¹¶è¡Œåœ°å‘å¤–æ‰©å±•ä¸€è·³é‚»å±…ï¼Œæ›´æ–°å›¾ç»“æ„å’Œå®ä½“ä¿¡æ¯
def explore_graph_from_one_topic_entities(current_entities, graph, entity_names, explored_entities, all_entities, limit_per_entity=100):
    """
    current_entities: set(str), å½“å‰è½®æ¬¡ä¸­è¦æ‰©å±•çš„å®ä½“é›†åˆï¼ˆFreebase MIDï¼‰
    graph: dict, å½“å‰æ­£åœ¨æ„å»ºçš„å›¾ç»“æ„
    entity_names: dict, MID â†’ å®ä½“åç§°
    exlored_entities: set(str), å·²æ‰€æœ‰å·²æ¢ç´¢è¿‡çš„å®ä½“ï¼Œé˜²æ­¢é‡å¤
    all_entities: set(str), æ‰€æœ‰å‘ç°çš„å®ä½“ï¼ˆå…¨å±€é›†åˆï¼‰
    answer_name: list(str), æ­£ç¡®ç­”æ¡ˆçš„å®ä½“åç§°
    """
    storage_lock = Lock()
    new_entities = set()

    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = {
            executor.submit(updated_search_relations_and_entities_combined_1, entity, limit_per_entity): entity
            for entity in current_entities
        }

        explored_entities.update(current_entities)

        for future in as_completed(futures):
            results = future.result()
            entity = futures[future]

            for result in results:
                rel = result["relation"]
                neighbor = result["connectedEntity"]
                name = result["connectedEntityName"]
                direction = result["direction"]

                if not neighbor.startswith("m.") or neighbor in explored_entities:
                    continue

                with storage_lock:
                    entity_names[neighbor] = name

                    if entity not in graph:
                        graph[entity] = {}

                    if neighbor not in graph:
                        graph[neighbor] = {}

                    if neighbor not in graph[entity]:
                        graph[entity][neighbor] = {"forward": set(), "backward": set()}
                        
                    if entity not in graph[neighbor]:
                        graph[neighbor][entity] = {"forward": set(), "backward": set()}

                    if direction == "tail":
                        graph[entity][neighbor]["forward"].add(rel)
                        graph[neighbor][entity]["backward"].add(rel)
                    else:
                        graph[entity][neighbor]["backward"].add(rel)
                        graph[neighbor][entity]["forward"].add(rel)

                new_entities.add(neighbor)

    new_entities.difference_update(explored_entities)
    all_entities.update(new_entities)
    current_entities = new_entities

    return graph, all_entities, explored_entities, current_entities, entity_names



# ä»ä¸€ä¸ªå®ä½“å‡ºå‘ï¼Œåœ¨ Freebase çŸ¥è¯†å›¾è°±ä¸­æŸ¥è¯¢ä¸ä¹‹è¿æ¥çš„æ‰€æœ‰ä¸‰å…ƒç»„ï¼Œå¹¶è¿”å›æ¸…æ´—åçš„ç»“æ„åŒ–ç»“æœã€‚
def updated_search_relations_and_entities_combined_1(entity_id, limit=100):
    """
    æŸ¥è¯¢å®ä½“çš„é‚»å±…ï¼Œä»…ä¿ç•™æœ‰ label çš„ï¼Œé™åˆ¶è¿”å›æ•°é‡ã€‚
    """
    query = f"""
    PREFIX ns: <http://rdf.freebase.com/ns/>
    SELECT DISTINCT ?relation ?connectedEntity ?connectedEntityName ?direction
    WHERE {{
        {{
            ns:{entity_id} ?relation ?connectedEntity .
            ?connectedEntity ns:type.object.name ?name .
            FILTER(lang(?name) = 'en')
            BIND(?name AS ?connectedEntityName)
            BIND("tail" AS ?direction)
        }}
        UNION
        {{
            ?connectedEntity ?relation ns:{entity_id} .
            ?connectedEntity ns:type.object.name ?name .
            FILTER(lang(?name) = 'en')
            BIND(?name AS ?connectedEntityName)
            BIND("head" AS ?direction)
        }}
    }}
    LIMIT {limit}
    """
    results = execute_sparql(query)
    return replace_prefix1(results)




# å°† http://rdf.freebase.com/ns/ å‰ç¼€æ¸…æ´—æ‰ï¼Œä¿ç•™ m.02jx3 è¿™æ ·çš„ç®€å†™ MIDã€‚
def replace_prefix1(data):
    if data is None:
        print("Warning: No data available to process in replace_prefix1.")
        return []

    # Returns a list of cleaned dictionaries.
    return [{key: value['value'].replace("http://rdf.freebase.com/ns/", "") for key, value in result.items()} for result in data]



def execute_sparql(sparql_txt):
    """
    Sends a SPARQL query to a Freebase/Wikidata endpoint and retrieves structured data in JSON format.
    Returns a list of dictionaries (from "bindings") or None if the query fails.
    """
    sparql = SPARQLWrapper(SPARQLPATH)
    sparql.setQuery("define sql:big-data-const 0\n" + sparql_txt)  # âœ… ä¸å†æ·»åŠ  LIMIT 3000
    sparql.setReturnFormat(JSON)
    
    attempts = 0
    while attempts < 3:
        try:
            results = sparql.query().convert()
            return results["results"]["bindings"]
        except Exception as e:
            print("404 Error encountered. Retrying after 2 seconds...")
            print(e)
            time.sleep(2)
            attempts += 1

    print("Failed to execute after multiple attempts.")
    return None



# åœ¨æœ€å¤š max_depth è·³å†…ï¼Œä»å¤šä¸ª topic entities åŒæ—¶å‡ºå‘ï¼Œæœç´¢é‚»æ¥å®ä½“ï¼Œå¹¶æ„å»ºä¸€ä¸ªåŒ…å«è¿™äº›å®ä½“ã€å…³ç³»ã€æ–¹å‘çš„å›¾ç»“æ„ï¼Œ
# ç›´åˆ°æ‰¾åˆ°æ‰€æœ‰ç­”æ¡ˆå®ä½“å¹¶åˆ¤æ–­è¿™äº› topic entities æ˜¯å¦äº’ç›¸è¿é€šã€‚
# entity_ids: å¤šä¸ª topic entity çš„ Freebase IDï¼ˆä¾‹å¦‚ "m.02jx3"ï¼‰
# answer_name: æ­£ç¡®ç­”æ¡ˆçš„åå­—åˆ—è¡¨ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦æ‰¾åˆ°ç­”æ¡ˆï¼‰
def explore_graph_from_entities_by_hop_neighbor_1(entity_ids, max_depth=5, answer_name=[]):
    current_entities = set(entity_ids)  # å½“å‰è¿™ä¸€å±‚çš„æ¢ç´¢èŠ‚ç‚¹
    all_entities = set(entity_ids)      # æ€»å…±è§è¿‡çš„èŠ‚ç‚¹
    found_answer = False                # æ˜¯å¦æ‰¾åˆ°æ‰€æœ‰ç­”æ¡ˆ

    entity_names = {entity: id2entity_name_or_type(entity) for entity in entity_ids}  # é»˜è®¤æ‰€æœ‰åˆå§‹å®ä½“åç§°ä¸º"unnamedentity"
    graph = {entity: {} for entity in all_entities}  # åˆå§‹åŒ–å›¾

    storage_lock = Lock()               # åˆ›å»ºçº¿ç¨‹å®‰å…¨é”
    
    answer_name_set = set(answer_name) 
    empty_set = set()                   # ç”¨äºåˆ¤æ–­å·²æ‰¾åˆ°çš„ç­”æ¡ˆå®ä½“
    if len(entity_ids) == 1:
        connect = True
    else:
        connect = False
    hopnumber = 5

    #  å¤šè½® BFSï¼šä»å¤šä¸ªå®ä½“ä¸€èµ·å‡ºå‘ï¼Œæ¢ç´¢é‚»å±…ç›´åˆ° max_depth
    for depth in range(1, max_depth + 1):
        print(f"Exploring entities at depth {depth}...")
        start = time.time()
        new_entities = set()

        # ç”¨å¤šçº¿ç¨‹å¹¶å‘æœç´¢é‚»å±…å®ä½“
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = {executor.submit(search_relations_and_entities_combined_1, entity): entity for entity in current_entities}

            # å¯¹è¿”å›çš„ç»“æœå¤„ç†å›¾ç»“æ„ + ç­›é€‰ç­”æ¡ˆ
            for future in as_completed(futures):
                results = future.result()
                entity = futures[future]
                for result in results:
                    relation, connected_entity, connected_name, direction = result['relation'], result['connectedEntity'], result['connectedEntityName'], result['direction']

                    if connected_entity.startswith("m."):
                        # æ£€æŸ¥ connected_name æ˜¯å¦åœ¨ç­”æ¡ˆé›†ä¸­ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ ‡è®°ä¸ºç­”æ¡ˆå®ä½“
                        if connected_name in answer_name_set:
                            empty_set.add(connected_entity)
                            if len(empty_set) == len(answer_name_set):
                                found_answer = True
                        # æ›´æ–°å›¾
                        with storage_lock:
                            # æ›´æ–°æˆ–æ·»åŠ å®ä½“åç§°
                            entity_names[connected_entity] = connected_name
                            # ç¡®ä¿å›¾ä¸­åŒ…å«ç›¸å…³å®ä½“å’Œå…³ç³»
                            if entity not in graph:
                                graph[entity] = {}
                            if connected_entity not in graph:
                                graph[connected_entity] = {}
                            if connected_entity not in graph[entity]:
                                graph[entity][connected_entity] = {'forward': set(), 'backward': set()}
                            if entity not in graph[connected_entity]:
                                graph[connected_entity][entity] = {'forward': set(), 'backward': set()}

                            # æ›´æ–°å…³ç³»
                            if direction == "tail":
                                graph[entity][connected_entity]['forward'].add(relation)
                                graph[connected_entity][entity]['backward'].add(relation)
                            else:  # direction is "head"
                                graph[entity][connected_entity]['backward'].add(relation)
                                graph[connected_entity][entity]['forward'].add(relation)
                        new_entities.add(connected_entity)


        new_entities.difference_update(all_entities)
        all_entities.update(new_entities)
        current_entities = new_entities
        end = time.time()
        print(f"Time taken to explore depth {depth}: {end - start:.2f} seconds")
        if connect == False:
            connect = are_entities_connected(graph, entity_ids, all_entities)
            if connect:
                print(f"All entities are connected within {depth} hops.")
                hopnumber = depth

        if found_answer and connect:
            return (found_answer, graph, hopnumber, all_entities, current_entities, entity_names, connect)

    print("Entities are not fully connected or answer entity not found within the maximum allowed hops.")
    return (found_answer, graph, hopnumber, all_entities, current_entities, entity_names, connect)



# æ‰€æœ‰ topic entities æ˜¯å¦å¤„äºåŒä¸€ä¸ªè¿é€šå­å›¾ä¸­
def are_entities_connected(graph, total_entities, all_entities):
    """
    Check if starting from the first entity in total_entities, all other entities in total_entities can be visited.
    graph: Dictionary with entity as key and another dictionary {connected_entity: {'forward': set(), 'backward': set()}} as value.
    total_entities: Set of initial entities to check connectivity from.
    """
    # å¦‚æœæ²¡æœ‰å®ä½“ï¼Œåˆ™è¿é€š
    if not total_entities:
        return True

    total_entities_set = set(total_entities)

    # å¦‚æœåªæœ‰ä¸€ä¸ªå®ä½“ï¼Œåˆ™è”é€š
    if len(total_entities_set) == 1:
        return True

    # ä»ä»»æ„ä¸€ä¸ªå®ä½“å‡ºå‘åš BFS
    start_entity = next(iter(total_entities_set))
    visited = set()
    queue = deque([start_entity])

    while queue:
        current = queue.popleft()
        if current not in visited:
            visited.add(current)
            # Early termination check
            if total_entities_set.issubset(visited):
                return True

            # Add connected entities to the queue
            for connected_entity, relations in graph[current].items():
                if connected_entity not in visited:
                    queue.append(connected_entity)

    # Final check in case not all entities are connected
    return False



# def retrieve_answer_from_kg_simpleqa(topic_mid, relation_keyword, expected_answer_text):
#     sparql = SPARQLWrapper(SPARQL_ENDPOINT)

#     # Step 1: è·å– topic_mid æ‰€æœ‰è°“è¯
#     query1 = f"""
#     PREFIX ns: <http://rdf.freebase.com/ns/>
#     SELECT DISTINCT ?p
#     FROM <http://freebase.com>
#     WHERE {{
#         ns:{topic_mid} ?p ?o .
#     }}
#     LIMIT 100
#     """
#     sparql.setQuery(query1)
#     sparql.setReturnFormat(JSON)
#     try:
#         results = sparql.query().convert()
#         predicates = [
#             b["p"]["value"].replace("http://rdf.freebase.com/ns/", "")
#             for b in results["results"]["bindings"]
#         ]
#     except Exception:
#         return None, None, None, [], {}

#     # Step 2: æ¨¡ç³ŠåŒ¹é… relation keyword
#     matched_preds = [p for p in predicates if relation_keyword.lower() in p.lower().split(".")[-1]]
#     tried_answers = {}

#     if not matched_preds:
#         return None, None, None, matched_preds, tried_answers

#     # Step 3: éå†è°“è¯ï¼Œæ‰¾å‡ºç¬¦åˆ expected_answer_text çš„ entity
#     for predicate in matched_preds:
#         query2 = f"""
#         PREFIX ns: <http://rdf.freebase.com/ns/>
#         SELECT DISTINCT ?mid ?name
#         FROM <http://freebase.com>
#         WHERE {{
#             ns:{topic_mid} ns:{predicate} ?mid .
#             OPTIONAL {{ ?mid ns:type.object.name ?name . FILTER (lang(?name) = "en") }}
#         }}
#         LIMIT 200
#         """
#         sparql.setQuery(query2)
#         sparql.setReturnFormat(JSON)

#         try:
#             res = sparql.query().convert()
#             answer_list = []
#             for b in res["results"]["bindings"]:
#                 mid = b["mid"]["value"].replace("http://rdf.freebase.com/ns/", "")
#                 label = b["name"]["value"].strip() if "name" in b else None
#                 answer_list.append((mid, label))

#                 if label and label.lower() == expected_answer_text.lower():
#                     return mid, label, predicate, matched_preds, tried_answers

#             tried_answers[predicate] = answer_list

#         except Exception:
#             continue

#     # Step 4: fallbackï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªéç©º relation çš„ç¬¬ä¸€ä¸ª entity
#     for pred, items in tried_answers.items():
#         for mid, label in items:
#             if label:  # âœ… åªè¿”å› label éç©ºçš„
#                 return mid, label, pred, matched_preds, tried_answers

#     return None, None, None, matched_preds, tried_answers



# def is_compound_node(mid):
#     # åˆ¤æ–­æŸä¸ªmidæ˜¯å¦æ˜¯å¤åˆç±»å‹èŠ‚ç‚¹ï¼ˆæ²¡æœ‰nameä½†æœ‰typeï¼‰
#     sparql = SPARQLWrapper(SPARQLPATH)
#     query = f"""
#     PREFIX ns: <http://rdf.freebase.com/ns/>
#     ASK {{
#       ns:{mid} ns:type.object.name ?name .
#     }}
#     """
#     sparql.setQuery(query)
#     sparql.setReturnFormat(JSON)
#     try:
#         has_name = sparql.query().convert()["boolean"]
#         return not has_name  # æ²¡æœ‰name â†’ å¯èƒ½æ˜¯compound
#     except Exception:
#         return False



# def search_entity_neighbors(mid, relation, include_compound=True):
#     sparql = SPARQLWrapper(SPARQLPATH)

#     # Forward + backward 1-hop neighbors
#     query = f"""
#     PREFIX ns: <http://rdf.freebase.com/ns/>
#     SELECT DISTINCT ?e
#     FROM <http://freebase.com>
#     WHERE {{
#         {{
#             ns:{mid} ns:{relation} ?e .
#         }}
#         UNION
#         {{
#             ?e ns:{relation} ns:{mid} .
#         }}
#     }}
#     """
#     sparql.setQuery(query)
#     sparql.setReturnFormat(JSON)

#     try:
#         results = sparql.query().convert()
#         neighbors = [
#             b["e"]["value"].replace("http://rdf.freebase.com/ns/", "")
#             for b in results["results"]["bindings"]
#             if b["e"]["value"].startswith("http://rdf.freebase.com/ns/m.")
#         ]
#     except Exception as e:
#         print(f"SPARQL query failed for {mid}, relation {relation}: {e}")
#         return []

#     final_neighbors = set()

#     for n in neighbors:
#         if not include_compound:
#             final_neighbors.add(n)
#             continue

#         if is_compound_node(n):
#             final_neighbors.add(n)  # âœ… ä¿ç•™ compound æœ¬èº«

#             # æŸ¥è¯¢ compound èŠ‚ç‚¹çš„ä¸¤ä¾§é‚»å±…ï¼ˆä¸æ­¢ forwardï¼Œè¿˜åŒ…æ‹¬ backwardï¼‰
#             query2 = f"""
#             PREFIX ns: <http://rdf.freebase.com/ns/>
#             SELECT DISTINCT ?e2
#             FROM <http://freebase.com>
#             WHERE {{
#                 {{
#                     ns:{n} ?r ?e2 .
#                     FILTER (STRSTARTS(STR(?e2), "http://rdf.freebase.com/ns/m."))
#                     FILTER (?e2 != ns:{mid})
#                 }}
#                 UNION
#                 {{
#                     ?e2 ?r ns:{n} .
#                     FILTER (STRSTARTS(STR(?e2), "http://rdf.freebase.com/ns/m."))
#                     FILTER (?e2 != ns:{mid})
#                 }}
#             }}
#             """
#             sparql.setQuery(query2)
#             sparql.setReturnFormat(JSON)
#             try:
#                 res2 = sparql.query().convert()
#                 for b in res2["results"]["bindings"]:
#                     e2 = b["e2"]["value"].replace("http://rdf.freebase.com/ns/", "")
#                     final_neighbors.add(e2)
#             except Exception as e:
#                 print(f"SPARQL compound expansion failed for {n}: {e}")
#         else:
#             final_neighbors.add(n)

#     return list(final_neighbors)




def search_entity_neighbors(mid, relation):
    sparql = SPARQLWrapper(SPARQLPATH)
    query = f"""
    PREFIX ns: <http://rdf.freebase.com/ns/>
    SELECT DISTINCT ?e
    FROM <http://freebase.com>
    WHERE {{
        {{
            ns:{mid} ns:{relation} ?e .
        }}
        UNION
        {{
            ?e ns:{relation} ns:{mid} .
        }}
    }}
    """
    sparql.setQuery(query)
    sparql.setReturnFormat(JSON)
    try:
        results = sparql.query().convert()
        neighbors = [
            b["e"]["value"].replace("http://rdf.freebase.com/ns/", "")
            for b in results["results"]["bindings"]
            if b["e"]["value"].startswith("http://rdf.freebase.com/ns/m.")
        ]
        return neighbors
    except Exception as e:
        print(f"SPARQL query failed for {mid}, relation {relation}: {e}")
        return []





def get_label_for_mids(mids):
    if isinstance(mids, str):
        mids = [mids]  # å¦‚æœä¼ å…¥çš„æ˜¯å•ä¸ªmidï¼Œè‡ªåŠ¨è½¬æ¢ä¸ºåˆ—è¡¨

    sparql = SPARQLWrapper(SPARQLPATH)
    values_clause = " ".join(f"ns:{mid}" for mid in mids)
    
    query = f"""
    PREFIX ns: <http://rdf.freebase.com/ns/>
    SELECT ?mid ?name
    WHERE {{
      VALUES ?mid {{ {values_clause} }}
      ?mid ns:type.object.name ?name .
      FILTER(lang(?name) = "en")
    }}
    """

    sparql.setQuery(query)
    sparql.setReturnFormat(JSON)

    labels = {}
    try:
        results = sparql.query().convert()
        for b in results["results"]["bindings"]:
            mid = b["mid"]["value"].replace("http://rdf.freebase.com/ns/", "")
            labels[mid] = b["name"]["value"]
    except Exception as e:
        print("SPARQL query failed:", e)

    return labels



def extract_mids(triple_lines):
    mid_pattern = re.compile(r'ns:(m\.[a-zA-Z0-9_]+)')
    mids = set()
    for line in triple_lines:
        for match in mid_pattern.findall(line):
            mids.add(match)
    return list(mids)



def resolve_entity_labels(mids):
    if not mids:
        return {}
    sparql = SPARQLWrapper(SPARQLPATH)
    mid_values = " ".join([f"ns:{mid}" for mid in mids])
    query = f"""
    PREFIX ns: <http://rdf.freebase.com/ns/>
    SELECT ?mid ?name
    FROM <http://freebase.com>
    WHERE {{
        VALUES ?mid {{ {mid_values} }}
        ?mid ns:type.object.name ?name .
        FILTER(lang(?name) = "en")
    }}
    """
    sparql.setQuery(query)
    sparql.setReturnFormat(JSON)
    try:
        results = sparql.query().convert()
    except Exception as e:
        print(f"âš ï¸ Label query failed: {e}")
        return {}

    label_map = {}
    for b in results["results"]["bindings"]:
        mid = b["mid"]["value"].replace("http://rdf.freebase.com/ns/", "")
        name = b["name"]["value"]
        label_map[mid] = name
    return label_map



def force_select_all_vars(original_sparql):
    lines = original_sparql.strip().split('\n')
    prefix_lines = [line for line in lines if line.startswith('PREFIX')]
    body_lines = [line for line in lines if not line.startswith('PREFIX')]
    full_body = "\n".join(body_lines)

    vars = get_all_variables(full_body)
    select_line = "SELECT DISTINCT " + " ".join("?" + v for v in vars)

    body_start = next((i for i, line in enumerate(body_lines) if line.strip().startswith('SELECT')), -1)
    if body_start != -1:
        body_lines[body_start] = select_line
    else:
        body_lines.insert(0, select_line)

    return "\n".join(prefix_lines + body_lines)



def resolve_entity_types(mids):
    if not mids:
        return {}

    sparql = SPARQLWrapper(SPARQLPATH)
    mid_values = " ".join([f"ns:{mid}" for mid in mids])
    query = f"""
    PREFIX ns: <http://rdf.freebase.com/ns/>
    SELECT ?mid ?type
    FROM <http://freebase.com>
    WHERE {{
        VALUES ?mid {{ {mid_values} }}
        ?mid ns:type.object.type ?type .
    }}
    """
    sparql.setQuery(query)
    sparql.setReturnFormat(JSON)
    try:
        results = sparql.query().convert()
    except Exception as e:
        print(f"âš ï¸ Type query failed: {e}")
        return {}

    type_map = defaultdict(set)
    for b in results["results"]["bindings"]:
        mid = b["mid"]["value"].replace("http://rdf.freebase.com/ns/", "")
        t = b["type"]["value"].replace("http://rdf.freebase.com/ns/", "")
        type_map[mid].add(t)
    return {k: list(v) for k, v in type_map.items()}



def annotate_triples(triple_lines, label_map):
    annotated = []
    for line in triple_lines:
        def repl(match):
            mid = match.group(1)
            label = label_map.get(mid, "UNKNOWN")
            return f"ns:{mid} ({label})"
        annotated.append(re.sub(r'ns:(m\.[a-zA-Z0-9_]+)', repl, line))
    return annotated



def get_all_variables(sparql_body):
    vars = set(re.findall(r'\?([a-zA-Z_][a-zA-Z0-9_]*)', sparql_body))
    return sorted(vars)



def force_select_all_vars(original_sparql):
    lines = original_sparql.strip().split('\n')
    prefix_lines = [line for line in lines if line.startswith('PREFIX')]
    body_lines = [line for line in lines if not line.startswith('PREFIX')]
    full_body = "\n".join(body_lines)

    vars = get_all_variables(full_body)
    select_line = "SELECT DISTINCT " + " ".join("?" + v for v in vars)

    body_start = next((i for i, line in enumerate(body_lines) if line.strip().startswith('SELECT')), -1)
    if body_start != -1:
        body_lines[body_start] = select_line
    else:
        body_lines.insert(0, select_line)

    return "\n".join(prefix_lines + body_lines)



def query_variable_bindings_all_vars(sparql_text):
    sparql = SPARQLWrapper(SPARQLPATH)
    sparql.setQuery(sparql_text)
    sparql.setReturnFormat(JSON)

    result_map = defaultdict(set)

    try:
        results = sparql.query().convert()
        for row in results["results"]["bindings"]:
            for var, val in row.items():
                if val["type"] == "uri" and val["value"].startswith("http://rdf.freebase.com/ns/"):
                    mid = val["value"].replace("http://rdf.freebase.com/ns/", "")
                    result_map[var].add(mid)
    except Exception as e:
        print(f"âš ï¸ SPARQL execution failed: {e}")

    return {var: list(mids) for var, mids in result_map.items()}



def construct_subgraph(dataset, data, topic_entity, entity_names, question_real_answer, depth):
    print("--------- Construct Subgraph ---------")
    explored_entities = set()                           # å·²ç»è¢«è®¿é—®è¿‡çš„å®ä½“
    next_explore_entities = set(topic_entity.keys())    # å½“å‰è¿™è½® BFS å°†è¦æ‰©å±•çš„å®ä½“ï¼ˆåˆå§‹ä¸º topic entitiesï¼‰
    all_entities = set(topic_entity.keys())             # è®°å½•å›¾ä¸­æ‰€æœ‰è®¿é—®åˆ°çš„å®ä½“
    gt_entities = set()

    # ä¸´æ—¶ç»´æŠ¤çš„å­å›¾ï¼ˆé‚»æ¥è¡¨ç»“æ„ï¼‰
    graph = {entity: {} for entity in topic_entity.keys()}

    # ------------------------- SimpleQA -------------------------
    if dataset == "simpleqa":
        for answer in question_real_answer:
            answer_label = answer['label']
            answer_mid = answer['mid']
            answer_relation = answer['relation']

            entity_names[answer_mid] = answer_label

            print(f"âœ”ï¸ æ·»åŠ ç­”æ¡ˆ: {answer_label} ({answer_mid}) via {answer_relation}")

            # æ”¾å…¥ explored_entities å’Œ all_entities
            explored_entities.add(answer_mid)
            all_entities.add(answer_mid)

            # å»ºç«‹å›¾ç»“æ„ä¸­å¯¹åº”çš„èŠ‚ç‚¹
            if answer_mid not in graph:
                graph[answer_mid] = {}

            for topic_id in topic_entity:
                if topic_id not in graph:
                    graph[topic_id] = {}
                if answer_mid not in graph[topic_id]:
                    graph[topic_id][answer_mid] = {'forward': set(), 'backward': set()}
                if topic_id not in graph[answer_mid]:
                    graph[answer_mid][topic_id] = {'forward': set(), 'backward': set()}

                # æ·»åŠ å…³ç³»ï¼ˆé»˜è®¤ä¸º topic -> answer æ˜¯ forwardï¼‰
                graph[topic_id][answer_mid]['forward'].add(answer_relation)
                graph[answer_mid][topic_id]['backward'].add(answer_relation)

    # ------------------------- WebQuestions -------------------------
    elif dataset == "webquestions":
        for answer in question_real_answer:
            answer_label = answer['label']
            answer_mid = answer['mid']
            answer_relation = answer['relation']

            entity_names[answer_mid] = answer_label

            print(f"âœ”ï¸ æ·»åŠ ç­”æ¡ˆ: {answer_label} ({answer_mid}) via {answer_relation}")

            # æ”¾å…¥ explored_entities å’Œ all_entities
            explored_entities.add(answer_mid)
            all_entities.add(answer_mid)

            # å»ºç«‹å›¾ç»“æ„ä¸­å¯¹åº”çš„èŠ‚ç‚¹
            if answer_mid not in graph:
                graph[answer_mid] = {}

            for topic_id in topic_entity:
                if topic_id not in graph:
                    graph[topic_id] = {}
                if answer_mid not in graph[topic_id]:
                    graph[topic_id][answer_mid] = {'forward': set(), 'backward': set()}
                if topic_id not in graph[answer_mid]:
                    graph[answer_mid][topic_id] = {'forward': set(), 'backward': set()}

                # æ·»åŠ å…³ç³»ï¼ˆé»˜è®¤ä¸º topic -> answer æ˜¯ forwardï¼‰
                graph[topic_id][answer_mid]['forward'].add(answer_relation)
                graph[answer_mid][topic_id]['backward'].add(answer_relation)

    # ------------------------- GrailQA -------------------------
    elif dataset == "grailqa":
        graph_query = data.get("graph_query", {})
        if not graph_query:
            print("âš ï¸ No graph_query found in this GrailQA item.")
            return graph, entity_names
        
        nid_to_id = {}   # nid â†’ å®ä½“ IDï¼ˆmid / friendly_nameï¼‰
        id_to_name = {}  # å®ä½“ ID â†’ å±•ç¤ºåç§°
        nid_type = {}    # nid â†’ node_type æ ‡è®°ï¼š'topic' / 'answer' / 'intermediate'

        # Step 1: å¤„ç†æ‰€æœ‰èŠ‚ç‚¹ï¼Œæ„é€  nid_to_id / id_to_name
        for node in graph_query.get("nodes", []):
            nid = node["nid"]
            raw_id = node["id"]
            friendly = node.get("friendly_name", raw_id)
            qnode_flag = node.get("question_node", 0)

            if raw_id.startswith("m."):  # topic entity
                nid_to_id[nid] = raw_id
                id_to_name[raw_id] = topic_entity.get(raw_id, friendly)  # topic_entity ä¸­å·²æœ‰ label
                # entity_names[raw_id] = id_to_name[raw_id]
                # all_entities.add(raw_id)
                # explored_entities.add(raw_id)
                nid_type[nid] = "topic"
                if raw_id not in graph:
                    graph[raw_id] = {}

            elif qnode_flag == 1:  # answer node
                nid_type[nid] = "answer"
                # å¤šä¸ª answer å®ä½“ä¼šæ˜ å°„åˆ°åŒä¸€ä¸ª nid
                # å…ˆä¸åŠ å®ä½“ï¼Œç­‰ä¸‹ç»Ÿä¸€åœ¨ edge å¤„ç†ä¸­å±•å¼€åˆ°å¤šä¸ªå®ä½“

            else:  # ä¸­é—´èŠ‚ç‚¹
                nid_to_id[nid] = friendly
                id_to_name[friendly] = friendly
                entity_names[friendly] = friendly
                nid_type[nid] = "intermediate"
                if friendly not in graph:
                    graph[friendly] = {}

        # Step 2: éå†è¾¹
        for edge in graph_query.get("edges", []):
            src_nid = edge["start"]
            dst_nid = edge["end"]
            rel = edge["relation"]
            rel_name = edge.get("friendly_name", rel)

            src_type = nid_type.get(src_nid)
            dst_type = nid_type.get(dst_nid)

            # Case 1: answer node å‚ä¸ï¼ˆéœ€è¦å±•å¼€ä¸ºå¤šä¸ªå®ä½“ï¼‰
            if src_type == "answer" or dst_type == "answer":
                for answer in data["answer"]:
                    ans_mid = answer["answer_argument"]
                    ans_name = answer["entity_name"]
                    entity_names[ans_mid] = ans_name
                    all_entities.add(ans_mid)
                    explored_entities.add(ans_mid)
                    if ans_mid not in graph:
                        graph[ans_mid] = {}

                    # æ›¿æ¢ answer node ä¸ºçœŸå®å®ä½“ ID
                    src_id = ans_mid if src_type == "answer" else nid_to_id[src_nid]
                    dst_id = ans_mid if dst_type == "answer" else nid_to_id[dst_nid]

                    # åç§°æ˜ å°„ï¼ˆä¸­é—´èŠ‚ç‚¹å¯èƒ½ç”¨ friendly nameï¼‰
                    h_label = entity_names.get(src_id, src_id)
                    t_label = entity_names.get(dst_id, dst_id)

                    # åˆå§‹åŒ–å›¾ç»“æ„
                    if dst_id not in graph[src_id]:
                        graph[src_id][dst_id] = {'forward': set(), 'backward': set()}
                    if src_id not in graph[dst_id]:
                        graph[dst_id][src_id] = {'forward': set(), 'backward': set()}

                    graph[src_id][dst_id]['forward'].add(rel)
                    graph[dst_id][src_id]['backward'].add(rel)

                    print(f"â¤ {src_id} ({h_label}) --{rel}--> {dst_id} ({t_label})") 

            else:
                h_id = nid_to_id[src_nid]
                t_id = nid_to_id[dst_nid]
                h_label = entity_names.get(h_id, h_id)
                t_label = entity_names.get(t_id, t_id)

                if h_id not in graph:
                    graph[h_id] = {}
                if t_id not in graph:
                    graph[t_id] = {}
                if t_id not in graph[h_id]:
                    graph[h_id][t_id] = {'forward': set(), 'backward': set()}
                if h_id not in graph[t_id]:
                    graph[t_id][h_id] = {'forward': set(), 'backward': set()}

                graph[h_id][t_id]['forward'].add(rel)
                graph[t_id][h_id]['backward'].add(rel)

                print(f"â¤ {h_id} ({h_label}) --{rel}--> {t_id} ({t_label})")

        # Step 3: æ›¿æ¢ä¸­é—´èŠ‚ç‚¹çš„ friendly name ä¸ºçœŸå® mid (ä¿®æ­£ç‰ˆ)
        # é¦–å…ˆï¼Œæ”¶é›†ä¸­é—´èŠ‚ç‚¹ä¸å®ƒçš„å…³è”è¾¹
        intermediate_edges = defaultdict(list)
        for edge in graph_query["edges"]:
            s, t = edge["start"], edge["end"]
            if nid_type.get(s) == "intermediate":
                intermediate_edges[nid_to_id[s]].append(edge)
            if nid_type.get(t) == "intermediate":
                intermediate_edges[nid_to_id[t]].append(edge)

        for friendly_node, edges in intermediate_edges.items():
            print(f"\nğŸ” æ›¿æ¢ä¸­é—´èŠ‚ç‚¹: {friendly_node}ï¼ˆå…± {len(edges)} æ¡è¾¹ï¼‰")

            # ç”¨äºå­˜å‚¨æ¯ä¸ªanswerä¸ä¸­é—´èŠ‚ç‚¹çš„é‚»å±…äº¤é›†ç»“æœ
            mid_candidates_per_answer = defaultdict(set)

            # æ ‡è®°å½“å‰ä¸­é—´èŠ‚ç‚¹æ˜¯å¦è¿æ¥äº† answer èŠ‚ç‚¹
            has_answer_node = False

            print(f"nid_to_id = {nid_to_id}")

            for edge in edges:
                src_nid = edge["start"]
                dst_nid = edge["end"]
                rel = edge["relation"]

                # æ‰¾åˆ°è¿æ¥çš„å¦ä¸€ä¸ªèŠ‚ç‚¹
                if nid_to_id.get(src_nid) == friendly_node:
                    other_nid = dst_nid
                else:
                    other_nid = src_nid
                other_type = nid_type[other_nid]

                if other_type == "answer":
                    has_answer_node = True
                    # åˆ†åˆ«å¤„ç†æ¯ä¸ª answer
                    for ans in data["answer"]:
                        ans_mid = ans["answer_argument"]
                        neighbors = set(search_entity_neighbors(ans_mid, rel))
                        if not mid_candidates_per_answer[ans_mid]:
                            mid_candidates_per_answer[ans_mid] = neighbors
                        else:
                            mid_candidates_per_answer[ans_mid] &= neighbors
                else:
                    other_mid = nid_to_id[other_nid]
                    neighbors = set(search_entity_neighbors(other_mid, rel))
                    # å¦‚æœè¿æ¥çš„æ˜¯éanswerèŠ‚ç‚¹ï¼Œåˆ™éœ€è¦å’Œæ‰€æœ‰å·²å­˜çš„answeråˆ†åˆ«è®¡ç®—äº¤é›†
                    if has_answer_node:
                        for ans_mid in mid_candidates_per_answer:
                            mid_candidates_per_answer[ans_mid] &= neighbors
                    else:
                        # å¦ä¸€ä¾§ä¹ŸéanswerèŠ‚ç‚¹ï¼Œç›´æ¥ç”¨ neighbors åˆå§‹åŒ–
                        if "single" not in mid_candidates_per_answer:
                            mid_candidates_per_answer["single"] = neighbors
                        else:
                            mid_candidates_per_answer["single"] &= neighbors

            # ç°åœ¨åˆ†åˆ«å¤„ç†æ¯ä¸ª answerï¼ˆæˆ–å•ä¸ªæƒ…å†µï¼‰çš„å€™é€‰èŠ‚ç‚¹
            for ans_mid, candidate_mids in mid_candidates_per_answer.items():
                if not candidate_mids:
                    print(f"âš ï¸ æ— å…±åŒå®ä½“æ›¿æ¢ {friendly_node} ä¸ {ans_mid}")
                    continue
                print(f"âœ… ä¸­é—´èŠ‚ç‚¹ {friendly_node} å¯¹äº {ans_mid} å€™é€‰å®ä½“ä¸º: {candidate_mids}")

                for mid in candidate_mids:
                    label = get_label_for_mids([mid]).get(mid, "Unknown")
                    entity_names[mid] = label
                    all_entities.add(mid)
                    explored_entities.add(mid)

                    # åˆå§‹åŒ– graph ä¸­æ–°èŠ‚ç‚¹
                    if mid not in graph:
                        graph[mid] = {}

                    # å°† friendly node çš„è¿æ¥è¿ç§»åˆ° mid èŠ‚ç‚¹ä¸Š
                    if friendly_node in graph:
                        for neighbor in graph[friendly_node]:
                            # â— å…³é”®ä¿®æ­£é€»è¾‘ â—
                            # å¦‚æœneighboræ˜¯ç­”æ¡ˆå®ä½“,åªå…è®¸å½“å‰çš„ans_midè¿æ¥
                            if neighbor in mid_candidates_per_answer:
                                if neighbor != ans_mid:
                                    continue  # è·³è¿‡ä¸å±äºå½“å‰midçš„ç­”æ¡ˆèŠ‚ç‚¹
                            for direction in ['forward', 'backward']:
                                for rel in graph[friendly_node][neighbor][direction]:
                                    # æ­£å‘è¿ç§»
                                    if neighbor not in graph[mid]:
                                        graph[mid][neighbor] = {'forward': set(), 'backward': set()}
                                    graph[mid][neighbor][direction].add(rel)

                                    # åå‘è¿ç§»ï¼ˆæ›´æ–°é‚»å±…æŒ‡å‘ midï¼‰
                                    if mid not in graph[neighbor]:
                                        graph[neighbor][mid] = {'forward': set(), 'backward': set()}
                                    opposite_direction = 'backward' if direction == 'forward' else 'forward'
                                    graph[neighbor][mid][opposite_direction].add(rel)

            # åˆ é™¤åŸå§‹ friendly name èŠ‚ç‚¹
            if friendly_node in graph:
                del graph[friendly_node]

            # åˆ é™¤å…¶ä»–èŠ‚ç‚¹ä¸­æŒ‡å‘ friendly_node çš„ä¿¡æ¯
            for node in graph:
                if friendly_node in graph[node]:
                    del graph[node][friendly_node]

            # ä» all_entities ä¸­ç§»é™¤
            if friendly_node in all_entities:
                all_entities.remove(friendly_node)

            # å¯é€‰ï¼šä» entity_names ä¸­ç§»é™¤
            if friendly_node in entity_names:
                del entity_names[friendly_node]

        # Step 4: å¦‚æœæŸä¸ª topic entity æ²¡æœ‰é‚»å±…ï¼Œåˆ™ç›´æ¥è¿æ¥åˆ°æ‰€æœ‰ç­”æ¡ˆ
        for topic_id in topic_entity:
            if topic_id not in graph or not graph[topic_id]:
                print(f"âš ï¸ Topic entity {topic_id} æ²¡æœ‰ä»»ä½•é‚»å±…ï¼Œå¼ºåˆ¶è¿æ¥ç­”æ¡ˆ")

                # å…ˆæ‰¾å‡ºä¸è¯¥ topic entity ç›¸å…³çš„è¾¹
                related_edges = []
                for edge in graph_query.get("edges", []):
                    s, t = edge["start"], edge["end"]
                    rel = edge["relation"]
                    if nid_to_id.get(s) == topic_id or nid_to_id.get(t) == topic_id:
                        related_edges.append(rel)

                # å¦‚æœæ‰¾ä¸åˆ°ç›¸å…³è¾¹ï¼Œè·³è¿‡ï¼ˆç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼‰
                if not related_edges:
                    print(f"âš ï¸ æ— æ³•æ‰¾åˆ° topic entity {topic_id} çš„ç›¸å…³å…³ç³»è¾¹ï¼Œè·³è¿‡")
                    continue

                for answer in data["answer"]:
                    ans_mid = answer["answer_argument"]
                    ans_name = answer["entity_name"]

                    # å¦‚æœç­”æ¡ˆå®ä½“è¿˜ä¸åœ¨å›¾ä¸­ï¼Œåˆå§‹åŒ–
                    if ans_mid not in graph:
                        graph[ans_mid] = {}
                    if topic_id not in graph:
                        graph[topic_id] = {}

                    # åˆå§‹åŒ–åŒå‘è¿æ¥
                    if ans_mid not in graph[topic_id]:
                        graph[topic_id][ans_mid] = {'forward': set(), 'backward': set()}
                    if topic_id not in graph[ans_mid]:
                        graph[ans_mid][topic_id] = {'forward': set(), 'backward': set()}

                    # å°†æ‰€æœ‰ç›¸å…³è¾¹éƒ½ç”¨ä¸Šï¼ˆæ³¨æ„å»é‡ï¼‰
                    for rel in set(related_edges):
                        graph[topic_id][ans_mid]['forward'].add(rel)
                        graph[ans_mid][topic_id]['backward'].add(rel)

                    print(f"ğŸ”— å¼ºè¿è¾¹: {topic_id} --{rel}--> {ans_mid} ({ans_name})")

    # ------------------------- cwq -------------------------
    elif dataset == "cwq":
        sparql_query = data.get("sparql", "")
        if not sparql_query:
            print("âš ï¸ No SPARQL query found in this CWQ item.")
            return graph, entity_names

        # æ”¹å†™SPARQLæŸ¥è¯¢ï¼Œæå–æ‰€æœ‰å˜é‡
        rewritten_sparql = force_select_all_vars(sparql_query)
        bindings = query_variable_bindings_all_vars(rewritten_sparql)

        # æ”¶é›†æ‰€æœ‰çš„ mid
        all_mids = {mid for mids in bindings.values() for mid in mids}

        # è·å–æ‰€æœ‰ mid çš„ labels å’Œ types
        label_map = resolve_entity_labels(all_mids)
        type_map = resolve_entity_types(all_mids)

        # æ›´æ–° entity_names, explored_entities, all_entities
        for mid in all_mids:
            label = label_map.get(mid)
            if not label:
                types = type_map.get(mid, ["UNKNOWN"])
                label = types[0]
            entity_names[mid] = label

            if mid not in topic_entity:
                explored_entities.add(mid)
                all_entities.add(mid)

            if mid not in graph:
                graph[mid] = {}

        # æå– SPARQL ä¸­çš„ triples
        triple_lines = [
            line.strip()
            for line in sparql_query.split('\n')
            if 'ns:' in line and not line.strip().startswith(('PREFIX', 'FILTER', '#'))
        ]

        for triple in triple_lines:
            parts = triple.split()
            if len(parts) < 3:
                continue

            subj, rel, obj = parts[0], parts[1], parts[2]
            subj = subj.replace('ns:', '')
            obj = obj.replace('ns:', '')
            rel = rel.replace('ns:', '')

            # å¤„ç†å˜é‡ (ä»¥?å¼€å¤´) çš„æƒ…å†µ
            subj_mids = bindings.get(subj[1:], []) if subj.startswith('?') else [subj]
            obj_mids = bindings.get(obj[1:], []) if obj.startswith('?') else [obj]

            for subj_mid in subj_mids:
                for obj_mid in obj_mids:
                    # ç¡®ä¿èŠ‚ç‚¹å­˜åœ¨äº graph
                    if subj_mid not in graph:
                        graph[subj_mid] = {}
                    if obj_mid not in graph:
                        graph[obj_mid] = {}

                    # åˆå§‹åŒ– graph ä¸­çš„è¿æ¥ç»“æ„
                    if obj_mid not in graph[subj_mid]:
                        graph[subj_mid][obj_mid] = {'forward': set(), 'backward': set()}
                    if subj_mid not in graph[obj_mid]:
                        graph[obj_mid][subj_mid] = {'forward': set(), 'backward': set()}

                    # æ·»åŠ å…³ç³»
                    graph[subj_mid][obj_mid]['forward'].add(rel)
                    graph[obj_mid][subj_mid]['backward'].add(rel)

                    print(f"â¤ {subj_mid} ({entity_names.get(subj_mid, 'UNKNOWN')}) --{rel}--> {obj_mid} ({entity_names.get(obj_mid, 'UNKNOWN')})")

        # Step é¢å¤–ä¿®å¤: å¤„ç†ç›´æ¥å‡ºç°åœ¨ triples ä¸­ä½†æœªåœ¨ bindings ä¸­çš„ mid
        extra_mids_in_triples = set()
        mid_pattern = re.compile(r'ns:(m\.[a-zA-Z0-9_]+)')

        for triple in triple_lines:
            extra_mids_in_triples.update(mid_pattern.findall(triple))

        # ä»…å¤„ç†å°šæœªè¢«å¤„ç†çš„ mids
        unprocessed_mids = extra_mids_in_triples - all_mids - set(topic_entity.keys())
        if unprocessed_mids:
            extra_labels = resolve_entity_labels(unprocessed_mids)
            extra_types = resolve_entity_types(unprocessed_mids)

            for mid in unprocessed_mids:
                label = extra_labels.get(mid)
                if not label:
                    types = extra_types.get(mid, ["UNKNOWN"])
                    label = types[0]

                entity_names[mid] = label
                explored_entities.add(mid)
                all_entities.add(mid)

                if mid not in graph:
                    graph[mid] = {}

                print(f"âœ… è¡¥å……æ·»åŠ æœªå¤„ç†çš„ä¸­é—´å®ä½“: {mid} ({label})")

    # ------------------------- WebQSP -------------------------
    elif dataset == "webqsp":
        parses = data.get("Parses", [])
        if not parses:
            print("âš ï¸ No Parses found in this WebQSP item.")
            return graph, entity_names

        for parse in parses:
            sparql_query = parse.get("Sparql", "")
            if not sparql_query:
                print("âš ï¸ No SPARQL query in this parse.")
                continue

            # æ”¹å†™SPARQLæŸ¥è¯¢ï¼Œæå–æ‰€æœ‰å˜é‡
            rewritten_sparql = force_select_all_vars(sparql_query)
            bindings = query_variable_bindings_all_vars(rewritten_sparql)

            # æ”¶é›†æ‰€æœ‰çš„ mid
            all_mids = {mid for mids in bindings.values() for mid in mids}

            # è·å–æ‰€æœ‰ mid çš„ labels å’Œ types
            label_map = resolve_entity_labels(all_mids)
            type_map = resolve_entity_types(all_mids)

            # æ›´æ–° entity_names, explored_entities, all_entities
            for mid in all_mids:
                label = label_map.get(mid)
                if not label:
                    types = type_map.get(mid, ["UNKNOWN"])
                    label = types[0]
                entity_names[mid] = label

                if mid not in topic_entity:
                    explored_entities.add(mid)
                    all_entities.add(mid)

                if mid not in graph:
                    graph[mid] = {}

            # æå– SPARQL ä¸­çš„ triples
            triple_lines = [
                line.strip()
                for line in sparql_query.split('\n')
                if 'ns:' in line and not line.strip().startswith(('PREFIX', 'FILTER', '#'))
            ]

            for triple in triple_lines:
                parts = triple.split()
                if len(parts) < 3:
                    continue

                subj, rel, obj = parts[0], parts[1], parts[2]
                subj = subj.replace('ns:', '')
                obj = obj.replace('ns:', '')
                rel = rel.replace('ns:', '')

                # å¤„ç†å˜é‡ (ä»¥?å¼€å¤´) çš„æƒ…å†µ
                subj_mids = bindings.get(subj[1:], []) if subj.startswith('?') else [subj]
                obj_mids = bindings.get(obj[1:], []) if obj.startswith('?') else [obj]

                for subj_mid in subj_mids:
                    for obj_mid in obj_mids:
                        # ç¡®ä¿èŠ‚ç‚¹å­˜åœ¨äº graph
                        if subj_mid not in graph:
                            graph[subj_mid] = {}
                        if obj_mid not in graph:
                            graph[obj_mid] = {}

                        # åˆå§‹åŒ– graph ä¸­çš„è¿æ¥ç»“æ„
                        if obj_mid not in graph[subj_mid]:
                            graph[subj_mid][obj_mid] = {'forward': set(), 'backward': set()}
                        if subj_mid not in graph[obj_mid]:
                            graph[obj_mid][subj_mid] = {'forward': set(), 'backward': set()}

                        # æ·»åŠ å…³ç³»
                        graph[subj_mid][obj_mid]['forward'].add(rel)
                        graph[obj_mid][subj_mid]['backward'].add(rel)

                        print(f"â¤ {subj_mid} ({entity_names.get(subj_mid, 'UNKNOWN')}) --{rel}--> {obj_mid} ({entity_names.get(obj_mid, 'UNKNOWN')})")

            # Step é¢å¤–ä¿®å¤: å¤„ç†ç›´æ¥å‡ºç°åœ¨ triples ä¸­ä½†æœªåœ¨ bindings ä¸­çš„ mid
            extra_mids_in_triples = set()
            mid_pattern = re.compile(r'ns:(m\.[a-zA-Z0-9_]+)')

            for triple in triple_lines:
                extra_mids_in_triples.update(mid_pattern.findall(triple))

            # ä»…å¤„ç†å°šæœªè¢«å¤„ç†çš„ mids
            unprocessed_mids = extra_mids_in_triples - all_mids - set(topic_entity.keys())
            if unprocessed_mids:
                extra_labels = resolve_entity_labels(unprocessed_mids)
                extra_types = resolve_entity_types(unprocessed_mids)

                for mid in unprocessed_mids:
                    label = extra_labels.get(mid)
                    if not label:
                        types = extra_types.get(mid, ["UNKNOWN"])
                        label = types[0]

                    entity_names[mid] = label
                    explored_entities.add(mid)
                    all_entities.add(mid)

                    if mid not in graph:
                        graph[mid] = {}

                    print(f"âœ… è¡¥å……æ·»åŠ æœªå¤„ç†çš„ä¸­é—´å®ä½“: {mid} ({label})")

    gt_entities = all_entities

    print(f"topic_entity = {topic_entity}")
    print(f"real_answers = {question_real_answer}")
    print(f"entity_names = {entity_names}")
    print(f"explored_entities = {explored_entities}")
    print(f"next_explore_entities = {next_explore_entities}")
    print(f"all_entities = {all_entities}")
    print(f"ground truth entities = {gt_entities}")

    graph = {k: v for k, v in graph.items() if k.startswith("m.")}

    print(f"\nå¼€å§‹æ·»åŠ é ground-truth éƒ¨åˆ†:)\n")
    for _ in range(depth):
        graph, all_entities, explored_entities, next_explore_entities, entity_names = explore_graph_from_one_topic_entities(
            next_explore_entities,
            graph,
            entity_names,
            explored_entities,
            all_entities,
            limit_per_entity=100 if dataset in {"simpleqa", "webquestions"} else 10
        )
    print(f"all_entities = {all_entities}")

    return graph, entity_names, gt_entities



